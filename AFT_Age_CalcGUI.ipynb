{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gs\n",
    "from scipy.stats import gamma\n",
    "from datetime import date\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, FileUpload, Button, Layout, Checkbox, HBox, VBox, Output\n",
    "from IPython.display import display, clear_output, Javascript\n",
    "\n",
    "#calls the CentralAge.py file (should be in the same folder as project)\n",
    "from CentralAge import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) DEFINE VARIABLES AND CHECK CONSTANTS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cb35f20a69443faf540c709f11f3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Select(description='RAp:', index=1, options=(('7.5e-4 (FCT)', 0.00075), ('7.17e-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912d570677e84731acdd6cb04d86a76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='primary', description='Reset', style=ButtonStyle()),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RAp_w = widgets.Select(\n",
    "    options=[(\"7.5e-4 (FCT)\",7.5E-4), (\"7.17e-4 (DUR)\",7.17E-4)],\n",
    "    value=7.17E-4,\n",
    "    rows=2,\n",
    "    description='RAp:',\n",
    "    disabled=False)\n",
    "\n",
    "qAp_w = widgets.FloatText(\n",
    "    value=0.93,\n",
    "    description='qAp:',\n",
    "    step=0.01,\n",
    "    disabled=False,)\n",
    "\n",
    "dAp_w = widgets.FloatText(\n",
    "    value=3.21,\n",
    "    description='dAp:',\n",
    "    step=0.01,\n",
    "    disabled=False)\n",
    "\n",
    "M238U_w = widgets.FloatText(\n",
    "    value=238.051,\n",
    "    description='M238U:',\n",
    "    step=0.01,\n",
    "    disabled=True)\n",
    "\n",
    "g_w = widgets.FloatText(\n",
    "    value=1.0,\n",
    "    description='g:',\n",
    "    step=0.01,\n",
    "    disabled=True)\n",
    "\n",
    "ld_w = widgets.FloatText(\n",
    "    value=1.55125E-10,\n",
    "    description='λd:',\n",
    "    step=0.01,\n",
    "    disabled=True)\n",
    "\n",
    "lf_w = widgets.FloatText(\n",
    "    value=8.52E-17,\n",
    "    description='λf:',\n",
    "    step=0.01,\n",
    "    disabled=True)\n",
    "\n",
    "No_w = widgets.FloatText(\n",
    "    value=6.0221409E+23,\n",
    "    description='No:',\n",
    "    step=0.01,\n",
    "    disabled=True)\n",
    "\n",
    "def Xi_f(RAp_w, qAp_w, dAp_w, M238U_w, g_w, ld_w, lf_w, No_w):\n",
    "    global Xi\n",
    "    global RAp\n",
    "    global qAp\n",
    "    global dAp\n",
    "    global M238U\n",
    "    global g\n",
    "    global ld\n",
    "    global lf\n",
    "    global No\n",
    "    Xi = M238U_w/(lf_w*No_w*dAp_w*RAp_w*qAp_w)\n",
    "    RAp = RAp_w\n",
    "    qAp = qAp_w\n",
    "    dAp = dAp_w\n",
    "    M238U = M238U_w\n",
    "    g = g_w\n",
    "    ld = ld_w\n",
    "    lf = lf_w\n",
    "    No = No_w\n",
    "    print ('Aggregate factor ξ:'\" {:.4e}\".format(Xi))\n",
    "\n",
    "def reset_values1(b):\n",
    "    \"\"\"Reset to inital values.\"\"\"\n",
    "    RAp_w.value = 7.17E-4\n",
    "    qAp_w.value = 0.93\n",
    "    dAp_w.value = 3.21\n",
    "\n",
    "reset_button1 = widgets.Button(description = \"Reset\", button_style='primary')\n",
    "reset_button1.on_click(reset_values1)\n",
    "\n",
    "print('1) DEFINE VARIABLES AND CHECK CONSTANTS')\n",
    "out1 = widgets.interactive_output(Xi_f, {'RAp_w': RAp_w, 'qAp_w': qAp_w, 'dAp_w': dAp_w, 'M238U_w': M238U_w,\n",
    "                                     'g_w': g_w,'ld_w': ld_w, 'lf_w': lf_w, 'No_w': No_w})\n",
    "display(widgets.HBox([widgets.VBox([RAp_w,qAp_w,dAp_w,out1]),\n",
    "                      widgets.VBox([M238U_w,g_w,ld_w,lf_w,No_w])]),\n",
    "       widgets.HBox([reset_button1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) OPTIONAL META DATA USED IN EXPORT TABLES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bb7356fe2244c69be5dcffe6ddf492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='Sample_01', description='Sample ID*:', placeholder='Sample_01'), Tex…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0643a370850944f2930d962a185466b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='primary', description='Default Values', style=ButtonStyle()), Button(butto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_name_w = widgets.Text(\n",
    "    value='Sample_01',\n",
    "    placeholder='Sample_01',\n",
    "    description='Sample ID*:',\n",
    "    disabled=False)\n",
    "\n",
    "analyst_w = widgets.Text(\n",
    "    value='M. McMillan',\n",
    "    placeholder='Your Name',\n",
    "    description='Analyst:',\n",
    "    disabled=False)\n",
    "\n",
    "collector_w = widgets.Text(\n",
    "    value='M. McMillan',\n",
    "    placeholder='Your Name?',\n",
    "    description='Collector:',\n",
    "    disabled=False)\n",
    "\n",
    "rock_type_w = widgets.Text(\n",
    "    #value='Hello World',\n",
    "    placeholder='Granite',\n",
    "    description='Rock Type:',\n",
    "    disabled=False)\n",
    "\n",
    "country_w = widgets.Text(\n",
    "    value='Malawi',\n",
    "    placeholder='Australia',\n",
    "    description='Country:',\n",
    "    disabled=False)\n",
    "\n",
    "region_w = widgets.Textarea(\n",
    "    #value='Hello World',\n",
    "    placeholder='Snowy Mountains',\n",
    "    description='Region:',\n",
    "    disabled=False)\n",
    "\n",
    "latitude_w = widgets.FloatText(\n",
    "    value=-12.3456,\n",
    "    step=None,\n",
    "    description='Lat. (wgs84):',\n",
    "    disabled=False)\n",
    "\n",
    "longitude_w = widgets.FloatText(\n",
    "    value=12.3456,\n",
    "    step=None,\n",
    "    description='Lon. (wgs84):',\n",
    "    disabled=False)\n",
    "\n",
    "elevation_w = widgets.FloatText(\n",
    "    #value=np.nan,\n",
    "    placeholder=100,\n",
    "    step=None,\n",
    "    description='elevation (m):',\n",
    "    disabled=False)\n",
    "\n",
    "    #These will likely remain constant\n",
    "\n",
    "mineral_w = widgets.Text(\n",
    "    value='Apatite',\n",
    "    placeholder='Apatite',\n",
    "    description='Mineral:',\n",
    "    disabled=False)\n",
    "\n",
    "Ustandard_w = widgets.Text(\n",
    "    value='Nist612',\n",
    "    placeholder='Standard',\n",
    "    description='U Standard:',\n",
    "    disabled=False)\n",
    "\n",
    "spot_size_w = widgets.Text(\n",
    "    value='30µm',\n",
    "    placeholder='30µm',\n",
    "    description='Spot Size:',\n",
    "    disabled=False)\n",
    "\n",
    "lab_name_w = widgets.Textarea(\n",
    "    value='University of Melbourne Thermochronology',\n",
    "    placeholder='University of ..',\n",
    "    description='Lab Name:',\n",
    "    disabled=False)\n",
    "\n",
    "etchant_w = widgets.Text(\n",
    "    value='5M HNO3',\n",
    "    placeholder='5M HNO3',\n",
    "    description='Etchant:',\n",
    "    disabled=False)\n",
    "\n",
    "etching_time_w = widgets.Text(\n",
    "    value='20s',\n",
    "    placeholder='20s',\n",
    "    description='Etch Time:',\n",
    "    disabled=False)\n",
    "\n",
    "etching_temp_w = widgets.Text(\n",
    "    value='20C',\n",
    "    placeholder='20C',\n",
    "    description='Etch Temp.:',\n",
    "    disabled=False)\n",
    "\n",
    "def meta_f(sample_name_w,analyst_w, collector_w, rock_type_w,\n",
    "           country_w, region_w, latitude_w, longitude_w, elevation_w,\n",
    "          mineral_w, Ustandard_w, spot_size_w, lab_name_w, etchant_w,\n",
    "          etching_time_w, etching_temp_w):\n",
    "    global sample_name\n",
    "    global analyst\n",
    "    global collector\n",
    "    global rock_type\n",
    "    global country\n",
    "    global region\n",
    "    global latitude\n",
    "    global longitude\n",
    "    global elevation\n",
    "    global mineral\n",
    "    global Ustandard\n",
    "    global spot_size\n",
    "    global lab_name\n",
    "    global etchant\n",
    "    global etching_time\n",
    "    global etching_temp\n",
    "    sample_name = sample_name_w\n",
    "    analyst = analyst_w\n",
    "    collector = collector_w\n",
    "    rock_type = rock_type_w\n",
    "    country = country_w\n",
    "    region = region_w\n",
    "    latitude = latitude_w\n",
    "    longitude = longitude_w\n",
    "    elevation = elevation_w\n",
    "    mineral = mineral_w\n",
    "    Ustandard = Ustandard_w\n",
    "    spot_size = spot_size_w\n",
    "    lab_name = lab_name_w\n",
    "    etchant = etchant_w\n",
    "    etching_time = etching_time_w\n",
    "    etching_temp = etching_temp_w\n",
    "    \n",
    "\n",
    "def reset_values2(b):\n",
    "    \"\"\"Clear to defined values.\"\"\"\n",
    "    sample_name_w.value = 'Sample_01'\n",
    "    analyst_w.value = ''\n",
    "    collector_w.value = ''\n",
    "    rock_type_w.value = ''\n",
    "    country_w.value = ''\n",
    "    region_w.value = ''\n",
    "    latitude_w.value = np.nan\n",
    "    longitude_w.value = np.nan\n",
    "    elevation_w.value = 0\n",
    "\n",
    "reset_button2 = widgets.Button(description = \"Clear Inputs\", button_style='danger')\n",
    "reset_button2.on_click(reset_values2)\n",
    "\n",
    "def reset_values3(b):\n",
    "    \"\"\"Reset to defined values.\"\"\"\n",
    "    mineral_w.value = \"Apatite\"\n",
    "    Ustandard_w.value = 'Nist612'\n",
    "    spot_size_w.value = '30µm'\n",
    "    lab_name_w.value = 'University of Melbourne Thermochronology'\n",
    "    etchant_w.value = '5M HNO3'\n",
    "    etching_time_w.value = '20s'\n",
    "    etching_temp_w.value = '20C'\n",
    "    sample_name_w.value = 'Sample_01'\n",
    "    analyst_w.value = 'M. McMillan'\n",
    "    collector_w.value = 'M. McMillan'\n",
    "    rock_type_w.value = ''\n",
    "    country_w.value = 'Malawi'\n",
    "    region_w.value = ''\n",
    "    latitude_w.value = -12.3456\n",
    "    longitude_w.value = 12.3456\n",
    "    elevation_w.value = 0\n",
    "\n",
    "reset_button3 = widgets.Button(description = \"Default Values\",button_style='primary')\n",
    "reset_button3.on_click(reset_values3)\n",
    "\n",
    "print(\"2) OPTIONAL META DATA USED IN EXPORT TABLES\")\n",
    "out2 = widgets.interactive_output(meta_f, {'sample_name_w': sample_name_w, 'analyst_w': analyst_w, 'collector_w': collector_w,\n",
    "                                           'rock_type_w': rock_type_w,'country_w': country_w,'region_w': region_w,\n",
    "                                           'latitude_w': latitude_w, 'longitude_w': longitude_w, 'elevation_w': elevation_w,\n",
    "                                          'mineral_w': mineral_w, 'Ustandard_w': Ustandard_w, 'spot_size_w': spot_size_w,\n",
    "                                          'lab_name_w': lab_name_w, 'etchant_w': etchant_w, 'etching_time_w': etching_time_w,'etching_temp_w': etching_temp_w})\n",
    "\n",
    "display(widgets.HBox([widgets.VBox([sample_name_w,analyst_w, collector_w, rock_type_w,\n",
    "           country_w, region_w, latitude_w, longitude_w, elevation_w]),\n",
    "                      \n",
    "              widgets.VBox([mineral_w, Ustandard_w, spot_size_w, lab_name_w, etchant_w,\n",
    "          etching_time_w, etching_temp_w])]),\n",
    "        \n",
    "        widgets.HBox([reset_button3,reset_button2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) TYPE DIRECTORY TO PATH WHERE FILES WILL BE SAVED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2efc8da56884b09868cc6b2faa04d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='/Users/malcmcm/Documents/Melbourne/PhD/Malawi/Python_OutputGUI', des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bf31bc717a4cd8a1203ceb0a5cdfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('3) TYPE DIRECTORY TO PATH WHERE FILES WILL BE SAVED')\n",
    "files_save_to_w = widgets.Text(\n",
    "    value='/Users/malcmcm/Documents/Melbourne/PhD/Malawi/Python_OutputGUI',\n",
    "    placeholder='/Users/Desktop/AFT_Calc',\n",
    "    description='Save To:',\n",
    "    layout=Layout(width='90%', height='100%'),\n",
    "    disabled=False)\n",
    "\n",
    "def create_folder(b):\n",
    "    global save_to\n",
    "    save_to = '{0}/FTAge_Calc/{1}'.format(files_save_to_w.value,sample_name)\n",
    "    with out_p:\n",
    "        print('Files will now save to:',save_to)\n",
    "    if not os.path.exists(save_to): os.makedirs(save_to)  \n",
    "save_button = widgets.Button(description = \"Store Path*\", button_style='primary')\n",
    "save_button.on_click(create_folder)\n",
    "\n",
    "def clear_folder(b):\n",
    "    files_save_to_w.value = \"\"\n",
    "    out_p.clear_output()\n",
    "clear_button = widgets.Button(description = \"Clear Path\", button_style='danger')\n",
    "clear_button.on_click(clear_folder)\n",
    "\n",
    "out_p = Output()\n",
    "\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([files_save_to_w]),widgets.HBox([save_button,clear_button])]))\n",
    "display(out_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) UPLOAD DATA FILES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35609b73ed7b458483b056ddbf948b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(FileUpload(value={}, accept='.csv', description='Counts (csv)*'), Output())), VB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~or~\n",
      "\n",
      " 4.1) TRY TO AUTO-RETRIEVE DATA FILES BASED ON SAMPLE NAME, Else: upload above\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c403d9295c143ddb1de18e21e151374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='/Users/malcmcm/Documents/Melbourne/PhD/Malawi/Final Data', descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea97a9691b543e9b0db0ff054e12633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91605c727f4d48059b7ffd29ce45e363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e181c532af4ebf86c0e938642f6000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Clear Files (reset)', layout=Layout(width='50%'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81210ea47470405fbf8bb6731ca783e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Commit Files & Continue', layout=Layout(width='50%'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough files to commit :( \n",
      ">>A counting .csv file and a ICPMS .txt file are required to continue\n",
      ">>>Did you forget to 'store' your files?\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.execute_cells([6,7,8])"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts_paths_test = []\n",
    "ICPMS_paths_test = []\n",
    "lengths_paths_test = []\n",
    "counts_paths = []\n",
    "ICPMS_paths = []\n",
    "lengths_paths = []\n",
    "\n",
    "counts_file_w = FileUpload(accept='.csv',\n",
    "                      description='Counts (csv)*',\n",
    "                      multiple=False)\n",
    "icpms_file_w = FileUpload(accept='.txt',\n",
    "                      description='ICPMS (txt)*',\n",
    "                      multiple=False)\n",
    "lengths_file_w = FileUpload(accept='.csv',\n",
    "                      description='Lengths (csv)',\n",
    "                      multiple=False)\n",
    "files_get_from_w = widgets.Text(\n",
    "    value='/Users/malcmcm/Documents/Melbourne/PhD/Malawi/Final Data',\n",
    "    placeholder='/Users/Desktop/AFT_Calc',\n",
    "    description='Look In:',\n",
    "    layout=Layout(width='90%', height='100%'),\n",
    "    disabled=False)\n",
    "\n",
    "def upload_c(counts_file_w):\n",
    "    if counts_file_w != {}:\n",
    "        print (next(iter(counts_file_w)))\n",
    "def upload_i(icpms_file_w):\n",
    "    if icpms_file_w != {}:\n",
    "        print (next(iter(icpms_file_w)))\n",
    "def upload_l(lengths_file_w):\n",
    "    if lengths_file_w != {}:\n",
    "        print (next(iter(lengths_file_w)))\n",
    "  \n",
    "    \n",
    "out_counts = widgets.interactive_output(upload_c, {'counts_file_w': counts_file_w})  \n",
    "out_icpms = widgets.interactive_output(upload_i, {'icpms_file_w': icpms_file_w}) \n",
    "out_lengths = widgets.interactive_output(upload_l, {'lengths_file_w': lengths_file_w}) \n",
    "\n",
    "def test_retrieve(b_test): \n",
    "    global counts_paths_test\n",
    "    counts_paths_test = []\n",
    "    for root, dirs, files in os.walk(files_get_from_w.value):\n",
    "        for f in files:\n",
    "            fullpath = os.path.join(root,f)\n",
    "            if \"ounts\" in fullpath and sample_name in fullpath and \".csv\" in fullpath:\n",
    "                counts_paths_test.append(fullpath)\n",
    "                with out_t:\n",
    "                    print (\"Track Counts File: {}\".format(counts_paths_test[0].split(\"/\")[-1]))\n",
    "    if counts_paths_test ==[]:\n",
    "        with out_t:\n",
    "            print('Searching in path.. {0}\\nNo File Matching Counts/{1}.csv found in path'.format(files_get_from_w.value,sample_name))\n",
    "    \n",
    "                \n",
    "    global ICPMS_paths_test\n",
    "    ICPMS_paths_test = []\n",
    "    for root, dirs, files in os.walk(files_get_from_w.value):\n",
    "        for f in files:\n",
    "            fullpath = os.path.join(root,f)\n",
    "            if \"ICPMS\" in fullpath and sample_name in fullpath and \".txt\" in fullpath:\n",
    "                ICPMS_paths_test.append(fullpath)\n",
    "                with out_t:\n",
    "                    print (\"ICPMS File: {}\".format(ICPMS_paths_test[0].split(\"/\")[-1]))\n",
    "    if ICPMS_paths_test ==[]:\n",
    "        with out_t:\n",
    "            print('No File Matching ICPMS/{0}.txt found in path'.format(sample_name))\n",
    "    \n",
    "                \n",
    "    global lengths_paths_test\n",
    "    lengths_paths_test = []\n",
    "    for root, dirs, files in os.walk(files_get_from_w.value):\n",
    "        for f in files:\n",
    "            fullpath = os.path.join(root,f)\n",
    "            if \"ength\" in fullpath and sample_name in fullpath and \".csv\" in fullpath:\n",
    "                lengths_paths_test.append(fullpath)\n",
    "                with out_t:\n",
    "                    print (\"Lengths File: {}\".format(lengths_paths_test[0].split(\"/\")[-1]))\n",
    "    if lengths_paths_test ==[]:\n",
    "        with out_t:\n",
    "            print('No File Matching Lengths/{0}.csv found in path'.format(sample_name))\n",
    "\n",
    "    if ICPMS_paths_test and counts_paths_test != []:\n",
    "        with out_t:\n",
    "            print ('>>Required Files Found, make sure to \"Store Files\" ')\n",
    "    \n",
    "def retrieve(b_ret): \n",
    "    global counts_paths\n",
    "    global ICPMS_paths\n",
    "    global lengths_paths\n",
    "    if counts_paths_test != []:\n",
    "        counts_paths = counts_paths_test[0]\n",
    "    else:\n",
    "        counts_paths = []\n",
    "    if ICPMS_paths_test != []:\n",
    "        ICPMS_paths = ICPMS_paths_test[0]\n",
    "    else:\n",
    "        ICPMS_paths = []\n",
    "    if lengths_paths_test != []:\n",
    "        lengths_paths = lengths_paths_test[0]\n",
    "    else:\n",
    "        lengths_paths = []\n",
    "    \n",
    "    with out_r:\n",
    "        if counts_paths != []:\n",
    "            print (\"Track Counts File (stored): {}\".format(counts_paths.split(\"/\")[-1]))\n",
    "        else:\n",
    "            print (\"No Counts File Found, Try testing the path first.\\n   .csv files should be in PATH/Counts/Sample_01.csv\")\n",
    "    with out_r:\n",
    "        if ICPMS_paths != []:\n",
    "            print (\"ICPMS File (stored): {}\".format(ICPMS_paths.split(\"/\")[-1]))\n",
    "        else:\n",
    "            print (\"No ICPMS File Found, Try testing the path first\\n    .txt files should be in PATH/ICPMS/Sample_01.csv\")\n",
    "    with out_r:\n",
    "        if lengths_paths != []:\n",
    "            print (\"Lengths File (stored): {}\".format(lengths_paths.split(\"/\")[-1]))\n",
    "        else:\n",
    "            print (\"No Lengths File Found, is that correct?\")\n",
    "    with out_r:\n",
    "        if ICPMS_paths_test and counts_paths_test != []:\n",
    "            print ('>>Looks Good! Commit and Continue')\n",
    "                    \n",
    "out_t = Output()\n",
    "out_r = Output()\n",
    "test_ret_button = widgets.Button(description = \"Test Path\", button_style='primary')\n",
    "test_ret_button.on_click(test_retrieve)\n",
    "\n",
    "ret_button = widgets.Button(description = \"Store Files\", button_style='primary')\n",
    "ret_button.on_click(retrieve)\n",
    "\n",
    "def clear_retrieve(b):\n",
    "    files_get_from_w.value = \"\"\n",
    "clear_button2 = widgets.Button(description = \"Clear Path\", button_style='danger')\n",
    "clear_button2.on_click(clear_retrieve)\n",
    "\n",
    "def clear_files(b):\n",
    "    out_t.clear_output()\n",
    "    out_r.clear_output()\n",
    "    global counts_paths_test\n",
    "    global ICPMS_paths_test\n",
    "    global lengths_paths_test\n",
    "    global counts_paths\n",
    "    global ICPMS_paths\n",
    "    global lengths_paths\n",
    "    counts_paths_test = []\n",
    "    ICPMS_paths_test = []\n",
    "    lengths_paths_test = []\n",
    "    counts_paths = []\n",
    "    ICPMS_paths = []\n",
    "    lengths_paths = []\n",
    "    out_counts.clear_output()\n",
    "    out_icpms.clear_output()\n",
    "    out_lengths.clear_output()\n",
    "    display(Javascript('IPython.notebook.execute_cell()'))\n",
    "clear_button3 = widgets.Button(description = \"Clear Files (reset)\",layout=Layout(width='50%'), button_style='danger')\n",
    "clear_button3.on_click(clear_files)\n",
    "\n",
    "def store_files(b):\n",
    "    if (ICPMS_paths == []) or (counts_paths == []):\n",
    "        print (\"Not enough files to commit :( \\n>>A counting .csv file and a ICPMS .txt file are required to continue\\n>>>Did you forget to 'store' your files?\")\n",
    "#    elif 'save_to' not in vars() or 'save_to' not in globals():\n",
    "#        print(\"Save Location is not defined!\\n >>'Store' save-to path in step 3.\")\n",
    "    else:\n",
    "        display(Javascript('IPython.notebook.execute_cells([6,7,8])'))\n",
    "store_button = widgets.Button(description = \"Commit Files & Continue\", layout=Layout(width='50%'),button_style='success')\n",
    "store_button.on_click(store_files) \n",
    "\n",
    "print(\"4) UPLOAD DATA FILES\")\n",
    "display(widgets.HBox([widgets.VBox([counts_file_w,out_counts]),\n",
    "              widgets.VBox([icpms_file_w,out_icpms]),\n",
    "             widgets.VBox([lengths_file_w,out_lengths])]))\n",
    "print('~or~\\n')\n",
    "print (' 4.1) TRY TO AUTO-RETRIEVE DATA FILES BASED ON SAMPLE NAME, Else: upload above')\n",
    "display(widgets.VBox([widgets.HBox([files_get_from_w]),widgets.HBox([test_ret_button,ret_button,clear_button2])]))\n",
    "display(out_t,out_r)\n",
    "display(widgets.HBox([clear_button3]))\n",
    "display(widgets.HBox([store_button]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5) READ THE UPLOADED FILES (Hit ~refresh~ button above)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'counts_file_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-85569a2f2096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Read Counts File##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5) READ THE UPLOADED FILES (Hit ~refresh~ button above)\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mcounts_file_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcounts_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_file_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_file_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcounts_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counts_file_w' is not defined"
     ]
    }
   ],
   "source": [
    "##Read Counts File##\n",
    "print(\"5) READ THE UPLOADED FILES (Hit ~refresh~ button above)\\n\")\n",
    "if counts_file_w.value != {}:\n",
    "    counts_content = counts_file_w.value[next(iter(counts_file_w.value))]['content']\n",
    "    counts_data = pd.read_csv(io.BytesIO(counts_content), skiprows=4)  \n",
    "        #AUTO-generate some meta-data from the counts file header\n",
    "    cd_headeronly = pd.read_csv(io.BytesIO(counts_content), error_bad_lines=False, header=None)\n",
    "    sample_ID = cd_headeronly[0][0] #currently this isnt used anywhere, but you could replace the variable sample_name with this\n",
    "    software = cd_headeronly[0][1]\n",
    "    px = pd.to_numeric(cd_headeronly[1][2])\n",
    "    py = pd.to_numeric(cd_headeronly[1][3])\n",
    "    date_meas = pd.to_datetime(cd_headeronly[2][0], infer_datetime_format=True)\n",
    "    print(\">>Counts file found, variables assigned :)\\nSoftware: {0}\\nDate Measured: {1}\\nSample: {2}\\nNo. Grains: {3}\\n_________________\".format(software,date_meas,sample_ID,counts_data['Grain/Mica'].count()))\n",
    "    #counts_data.head()\n",
    "elif counts_file_w.value == {} and counts_paths != []:\n",
    "    counts_data = pd.read_csv(counts_paths, skiprows=4)  \n",
    "        #AUTO-generate some meta-data from the counts file header\n",
    "    cd_headeronly = pd.read_csv(counts_paths, error_bad_lines=False, header=None)\n",
    "    sample_ID = cd_headeronly[0][0] #currently this isnt used anywhere, but you could replace the variable sample_name with this\n",
    "    software = cd_headeronly[0][1]\n",
    "    px = pd.to_numeric(cd_headeronly[1][2])\n",
    "    py = pd.to_numeric(cd_headeronly[1][3])\n",
    "    date_meas = pd.to_datetime(cd_headeronly[2][0], infer_datetime_format=True)\n",
    "    print(\">>Counts file found, variables assigned :)\\nSoftware: {0}\\nDate Measured: {1}\\nSample: {2}\\nNo. Grains: {3}\\n_________________\".format(software,date_meas,sample_ID,counts_data['Grain/Mica'].count()))\n",
    "    #counts_data.head()\n",
    "else:\n",
    "    print('>>>>>No Counts File Found!\\nThis is required!\\n_________________')\n",
    "    \n",
    "##Read ICPMS File##\n",
    "if icpms_file_w.value != {}:\n",
    "    icpms_content = icpms_file_w.value[next(iter(icpms_file_w.value))]['content']\n",
    "        #Selects only the columns we need\n",
    "    icpms_data = pd.read_csv(io.BytesIO(icpms_content),delimiter=\"\\t\",\n",
    "        usecols=[\"Unnamed: 0\", \"U_ppm_m238\", \"U_ppm_m238_Int2SE\", \"Time\",\n",
    "                 \"Ca43_CPS\", \"Ca43_CPS_Int2SE\", \"Th_ppm_m232\", \"Th_ppm_m232_Int2SE\"],)\n",
    "        #Renames the unnamed column at column 0\n",
    "    icpms_data.rename( columns={'Unnamed: 0':'point_name'}, inplace=True )\n",
    "        # need to convert certain columns to floats instead of string for plotting, also fill nan with 0\n",
    "    cols_numeric = [\"U_ppm_m238\", \"U_ppm_m238_Int2SE\", \"Ca43_CPS\", \"Ca43_CPS_Int2SE\",\n",
    "                    \"Th_ppm_m232\", \"Th_ppm_m232_Int2SE\"]\n",
    "    icpms_data[cols_numeric] = icpms_data[cols_numeric].apply(pd.to_numeric, errors='coerce', axis=1).fillna(0)\n",
    "    ####DEFINE THE STANDARDS USED BASED ON POINT NAMES####\n",
    "    #Information for standards used, you might need to add more if you've used other ones\n",
    "    std_dur = icpms_data[icpms_data['point_name'].str.contains('Dur', na=False, case=False)]\n",
    "    std_612 = icpms_data[icpms_data['point_name'].str.contains('612', na=False, case=False)]\n",
    "    std_614 = icpms_data[icpms_data['point_name'].str.contains('614', na=False, case=False)]\n",
    "    std_mt = icpms_data[icpms_data['point_name'].str.contains('Mud', na=False, case=False)]\n",
    "    output = icpms_data[icpms_data['point_name'].str.contains('Output', na=False, case=False)]\n",
    "    #resets the index row number for each standards data frame\n",
    "    std_dur.reset_index(drop=True, inplace=True)\n",
    "    std_612.reset_index(drop=True, inplace=True)\n",
    "    std_614.reset_index(drop=True, inplace=True)\n",
    "    std_mt.reset_index(drop=True, inplace=True)\n",
    "    output.reset_index(drop=True, inplace=True)\n",
    "        #combines the counts data and icpms data at the correct location to align\n",
    "    age_df = pd.concat([counts_data, output], axis=1)\n",
    "        #replaces the word \"Grain\" before each grain to make it easier to handle later\n",
    "    age_df['Grain/Mica'].replace(regex=True,inplace=True,to_replace='Grain',value='')\n",
    "        #see the data header if you want> (remove .head() to see full data)\n",
    "    #age_df\n",
    "    print(\">>ICPMS file found, variables assigned :)\\nFirst Unknown: {0}\\nNo. of Unknowns: {1}\\n_________________\".format(output['point_name'][0],output['point_name'].count()))\n",
    "elif icpms_file_w.value == {} and ICPMS_paths != []:\n",
    "        #Selects only the columns we need\n",
    "    icpms_data = pd.read_csv(ICPMS_paths,delimiter=\"\\t\",\n",
    "        usecols=[\"Unnamed: 0\", \"U_ppm_m238\", \"U_ppm_m238_Int2SE\", \"Time\",\n",
    "                 \"Ca43_CPS\", \"Ca43_CPS_Int2SE\", \"Th_ppm_m232\", \"Th_ppm_m232_Int2SE\"],)\n",
    "        #Renames the unnamed column at column 0\n",
    "    icpms_data.rename( columns={'Unnamed: 0':'point_name'}, inplace=True )\n",
    "        # need to convert certain columns to floats instead of string for plotting, also fill nan with 0\n",
    "    cols_numeric = [\"U_ppm_m238\", \"U_ppm_m238_Int2SE\", \"Ca43_CPS\", \"Ca43_CPS_Int2SE\",\n",
    "                    \"Th_ppm_m232\", \"Th_ppm_m232_Int2SE\"]\n",
    "    icpms_data[cols_numeric] = icpms_data[cols_numeric].apply(pd.to_numeric, errors='coerce', axis=1).fillna(0)\n",
    "    ####DEFINE THE STANDARDS USED BASED ON POINT NAMES####\n",
    "    #Information for standards used, you might need to add more if you've used other ones\n",
    "    std_dur = icpms_data[icpms_data['point_name'].str.contains('Dur', na=False, case=False)]\n",
    "    std_612 = icpms_data[icpms_data['point_name'].str.contains('612', na=False, case=False)]\n",
    "    std_614 = icpms_data[icpms_data['point_name'].str.contains('614', na=False, case=False)]\n",
    "    std_mt = icpms_data[icpms_data['point_name'].str.contains('Mud', na=False, case=False)]\n",
    "    output = icpms_data[icpms_data['point_name'].str.contains('Output', na=False, case=False)]\n",
    "    #resets the index row number for each standards data frame\n",
    "    std_dur.reset_index(drop=True, inplace=True)\n",
    "    std_612.reset_index(drop=True, inplace=True)\n",
    "    std_614.reset_index(drop=True, inplace=True)\n",
    "    std_mt.reset_index(drop=True, inplace=True)\n",
    "    output.reset_index(drop=True, inplace=True)\n",
    "        #combines the counts data and icpms data at the correct location to align\n",
    "    age_df = pd.concat([counts_data, output], axis=1)\n",
    "        #replaces the word \"Grain\" before each grain to make it easier to handle later\n",
    "    age_df['Grain/Mica'].replace(regex=True,inplace=True,to_replace='Grain',value='')\n",
    "        #see the data header if you want> (remove .head() to see full data)\n",
    "    #age_df\n",
    "    print(\">>ICPMS file found, variables assigned :)\\nFirst Unknown: {0}\\nNo. of Unknowns: {1}\\n_________________\".format(output['point_name'][0],output['point_name'].count()))\n",
    "else:\n",
    "    print('No ICPMS File Found!\\nThis is required!\\n_________________')\n",
    "    \n",
    "##Read Lengths File##\n",
    "if lengths_file_w.value != {}:\n",
    "    lengths_content = lengths_file_w.value[next(iter(lengths_file_w.value))]['content']\n",
    "    lengths_data = pd.read_csv(io.BytesIO(lengths_content), skiprows=4)\n",
    "    true_length = lengths_data[\"True Length\"]\n",
    "    mtl = true_length.mean()\n",
    "    mtl_sd = true_length.std()\n",
    "    l_no = true_length.count()\n",
    "    mtl_var = mtl_sd/np.sqrt(l_no)\n",
    "    Dpar_lengths = lengths_data[\"Average DPar(µmm)\"]\n",
    "    Dpar_lengths_mean = Dpar_lengths.mean()\n",
    "    rmr0D_lengths = round(0.84*((4.58-(0.9231*Dpar_lengths+0.2515))/2.98)**(0.21),4)\n",
    "    rmr0D_lengths_mean = rmr0D_lengths.mean()\n",
    "    rmr0D_lengths_sdm = rmr0D_lengths.std()\n",
    "    print(\">>Lengths file found, variables assigned :)\\nMTL: {0:.2f}±{1:.1f}\\nNo. Lengths: {2}\".format(mtl,mtl_sd,l_no))\n",
    "elif lengths_file_w.value == {} and lengths_paths !=[]:\n",
    "    lengths_data = pd.read_csv(lengths_paths, skiprows=4)\n",
    "    true_length = lengths_data[\"True Length\"]\n",
    "    mtl = true_length.mean()\n",
    "    mtl_sd = true_length.std()\n",
    "    l_no = true_length.count()\n",
    "    mtl_var = mtl_sd/np.sqrt(l_no)\n",
    "    Dpar_lengths = lengths_data[\"Average DPar(µmm)\"]\n",
    "    Dpar_lengths_mean = Dpar_lengths.mean()\n",
    "    rmr0D_lengths = round(0.84*((4.58-(0.9231*Dpar_lengths+0.2515))/2.98)**(0.21),4)\n",
    "    rmr0D_lengths_mean = rmr0D_lengths.mean()\n",
    "    rmr0D_lengths_sdm = rmr0D_lengths.std()\n",
    "    print(\">>Lengths file found, variables assigned :)\\nMTL: {0:.2f}±{1:.1f}\\nNo. Lengths: {2}\".format(mtl,mtl_sd,l_no))\n",
    "\n",
    "else:\n",
    "    print(\">>No lengths file found\\n_________________\")\n",
    "    lengths_content = '--'\n",
    "    lengths_data = '--'\n",
    "    true_length = '--'\n",
    "    mtl = '--'\n",
    "    mtl_sd = '--'\n",
    "    l_no = '--'\n",
    "    mtl_var = '--'\n",
    "    Dpar_lengths = '--'\n",
    "    Dpar_lengths_mean = '--'\n",
    "    rmr0D_lengths = '--'\n",
    "    rmr0D_lengths_mean = '--'\n",
    "    rmr0D_lengths_sdm = '--'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counts_file_w.value != {} or counts_paths != []:\n",
    "    ####PLOT SOME FIGURES FROM THE ICPMS DATA####\n",
    "\n",
    "    #define the accepted values for standards to use in plots\n",
    "    dur_value=12.2\n",
    "    dur_err=0.04\n",
    "    mt_value=3\n",
    "    mt_err=0.02\n",
    "    nist612_value=37.38\n",
    "    nist612_err=0.08\n",
    "    nist614_value=0.823\n",
    "    nist614_err=0.002\n",
    "\n",
    "    #####DURNAGO\n",
    "    #convert spot time to seconds\n",
    "    df_time_dur = pd.to_datetime(std_dur[\"Time\"])\n",
    "    x_dur = ((df_time_dur.dt.hour*60+df_time_dur.dt.minute)*60 + df_time_dur.dt.second)*10**(-4)\n",
    "    #find and define the Uppm\n",
    "    y_dur = std_dur[\"U_ppm_m238\"]\n",
    "    y_edur = std_dur[\"U_ppm_m238_Int2SE\"]\n",
    "    #define the mean line of the Uppm±SD\n",
    "    y_mdur = [np.mean(y_dur)]*len(x_dur)\n",
    "    y_medur = [np.mean(y_edur)]*len(x_dur)\n",
    "\n",
    "    #####MUD_TANK\n",
    "    #convert spot time to seconds\n",
    "    df_time_mt = pd.to_datetime(std_mt[\"Time\"])\n",
    "    x_mt = ((df_time_mt.dt.hour*60+df_time_mt.dt.minute)*60 + df_time_mt.dt.second)*10**(-4)\n",
    "    #find and define the Uppm\n",
    "    y_mt = std_mt[\"U_ppm_m238\"]\n",
    "    y_emt = std_mt[\"U_ppm_m238_Int2SE\"]\n",
    "    #define the mean line of the Uppm±SD\n",
    "    y_mmt = [np.mean(y_mt)]*len(x_mt)\n",
    "    y_memt = [np.mean(y_emt)]*len(x_mt)\n",
    "\n",
    "    ######OUTPUT(SAMPLES)\n",
    "    #convert spot time to seconds\n",
    "    df_time_output = pd.to_datetime(output[\"Time\"])\n",
    "    x_output = ((df_time_output.dt.hour*60+df_time_output.dt.minute)*60 + df_time_output.dt.second)*10**(-4)\n",
    "    #find and define the Uppm\n",
    "    y_output = output[\"U_ppm_m238\"]\n",
    "    y_eoutput = output[\"U_ppm_m238_Int2SE\"]\n",
    "    #define the mean line of the Uppm±SD\n",
    "    y_moutput = [np.mean(y_output)]*len(x_output)\n",
    "    y_meoutput = [np.mean(y_eoutput)]*len(x_output)\n",
    "\n",
    "    #####NIST612\n",
    "    #convert spot time to seconds\n",
    "    df_time_nist612 = pd.to_datetime(std_612[\"Time\"])\n",
    "    x_nist612 = ((df_time_nist612.dt.hour*60+df_time_nist612.dt.minute)*60 + df_time_nist612.dt.second)*10**(-4)\n",
    "    #find and define the Uppm\n",
    "    y_nist612 = std_612[\"U_ppm_m238\"]\n",
    "    y_enist612 = std_612[\"U_ppm_m238_Int2SE\"]\n",
    "    #define the mean line of the Uppm±SD\n",
    "    y_mnist612 = [np.mean(y_nist612)]*len(x_nist612)\n",
    "    y_menist612 = [np.mean(y_enist612)]*len(x_nist612)\n",
    "\n",
    "    if len(std_614['point_name']) != 0:\n",
    "            #####NIST614\n",
    "            #convert spot time to seconds\n",
    "        df_time_nist614 = pd.to_datetime(std_614[\"Time\"])\n",
    "        x_nist614 = ((df_time_nist614.dt.hour*60+df_time_nist614.dt.minute)*60 + df_time_nist614.dt.second)*10**(-4)\n",
    "            #find and define the Uppm\n",
    "        y_nist614 = std_614[\"U_ppm_m238\"]\n",
    "        y_enist614 = std_614[\"U_ppm_m238_Int2SE\"]\n",
    "            #define the mean line of the Uppm±SD\n",
    "        y_mnist614 = [np.mean(y_nist614)]*len(x_nist614)\n",
    "        y_menist614 = [np.mean(y_enist614)]*len(x_nist614)\n",
    "\n",
    "\n",
    "    #####FIGURES######\n",
    "    #setup the figure (fig size is in inches)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(9,9))\n",
    "\n",
    "    if len(std_614['point_name']) == 0:\n",
    "            ####PLOT NIST612 if NIST614 was not shot\n",
    "            #create a color span on plot representing the accepted value for the standard\n",
    "        axs[0,1].axhspan(nist612_value-nist612_err, nist612_value+nist612_err, alpha=0.1, color='cornflowerblue')\n",
    "            #plot the measured mean Uppm line in ref material\n",
    "        axs[0,1].plot(x_nist612,y_mnist612, label='Mean', linestyle='-', color='#8B0000', alpha=0.5)\n",
    "        axs[0,1].text(np.mean(x_nist612), np.mean(y_mnist612) + np.mean(y_mnist612)*0.01,'mean: {0:.2f}'\" \"u'\\xb1'\" \"'{1:.2f}'\" \"'ppm' .format(round(y_mnist612[0], 2),round(y_menist612[0], 2)),fontsize=11, ha='center', color='#8B0000', zorder=3)\n",
    "        axs[0,1].plot(np.unique(x_nist612), np.poly1d(np.polyfit(x_nist612, y_nist612, 1))(np.unique(x_nist612)), linestyle='--', color='k', alpha=0.2)\n",
    "            #plot the scatter points with error bars\n",
    "        axs[0,1].scatter(x_nist612,y_nist612, label='Data', marker='o', color='#8B0000', s=25, zorder=2, alpha=.5)\n",
    "        axs[0,1].errorbar(x_nist612,y_nist612, yerr=y_enist612, linestyle=\"None\", color='k', alpha=.3, zorder=1)\n",
    "            #title and x,y labels\n",
    "        axs[0,1].set_title('Nist-612', fontsize=12)\n",
    "    else:\n",
    "            ####Plot NIST614 if it was shot\n",
    "            #create a color span on plot representing the accepted value for the standard\n",
    "        axs[0,1].axhspan(nist614_value-nist614_err, nist614_value+nist614_err, alpha=0.1, color='cornflowerblue')\n",
    "            #plot the measured mean Uppm line in ref material\n",
    "        axs[0,1].plot(x_nist614,y_mnist614, label='Mean', linestyle='-', color='#8B0000', alpha=0.5)\n",
    "        axs[0,1].text(np.mean(x_nist614), np.mean(y_mnist614) + np.mean(y_mnist614)*0.01,'mean: {0:.2f}'\" \"u'\\xb1'\" \"'{1:.2f}'\" \"'ppm' .format(round(y_mnist614[0], 2),round(y_menist614[0], 2)),fontsize=11, ha='center', color='#8B0000', zorder=3)\n",
    "        axs[0,1].plot(np.unique(x_nist614), np.poly1d(np.polyfit(x_nist614, y_nist614, 1))(np.unique(x_nist614)), linestyle='--', color='k', alpha=0.2)\n",
    "            #plot the scatter points with error bars\n",
    "        axs[0,1].scatter(x_nist614,y_nist614, label='Data', marker='o', color='#8B0000', s=25, zorder=2, alpha=.5)\n",
    "        axs[0,1].errorbar(x_nist614,y_nist614, yerr=y_enist614, linestyle=\"None\", color='k', alpha=.3, zorder=1)\n",
    "            #title and x,y labels\n",
    "        axs[0,1].set_title('Nist-614', fontsize=12)\n",
    "\n",
    "    ####DURANGO\n",
    "    #create a color span on plot representing the accepted value for the standard\n",
    "    axs[1,0].axhspan(dur_value-dur_err, dur_value+dur_err, alpha=0.1, color='cornflowerblue')\n",
    "    #plot the measured mean Uppm line in ref material\n",
    "    axs[1,0].plot(x_dur,y_mdur, label='Mean', linestyle='-', color='#8B0000', alpha=0.5)\n",
    "    axs[1,0].text(np.mean(x_dur), np.mean(y_mdur) + np.mean(y_mdur)*0.01,'mean: {0:.2f}'\" \"u'\\xb1'\" \"'{1:.2f}'\" \"'ppm' .format(round(y_mdur[0], 2),round(y_medur[0], 2)),fontsize=11, ha='center', color='#8B0000', zorder=3)\n",
    "    axs[1,0].plot(np.unique(x_dur), np.poly1d(np.polyfit(x_dur, y_dur, 1))(np.unique(x_dur)), linestyle='--', color='k', alpha=0.2)\n",
    "    #plot the scatter points with error bars\n",
    "    axs[1,0].scatter(x_dur,y_dur, label='Data', marker='o', color='#8B0000', s=25, zorder=2, alpha=.5)\n",
    "    axs[1,0].errorbar(x_dur,y_dur, yerr=y_edur, linestyle=\"None\", color='k', alpha=.3, zorder=1)\n",
    "    #title and x,y labels\n",
    "    axs[1,0].set_title('Durango', fontsize=12)\n",
    "\n",
    "    ####MUD_TANK\n",
    "    #create a color span on plot representing the accepted value for the standard\n",
    "    axs[1,1].axhspan(mt_value-mt_err, mt_value+mt_err, alpha=0.1, color='cornflowerblue')\n",
    "    #plot the measured mean Uppm line in ref material\n",
    "    axs[1,1].plot(x_mt,y_mmt, label='Mean', linestyle='-', color='#8B0000', alpha=0.5)\n",
    "    axs[1,1].text(np.mean(x_mt), np.mean(y_mmt) + np.mean(y_mmt)*0.015,'mean: {0:.2f}'\" \"u'\\xb1'\" \"'{1:.2f}'\" \"'ppm' .format(round(y_mmt[0], 2),round(y_memt[0], 2)),fontsize=11, ha='center', color='#8B0000', zorder=3)\n",
    "    axs[1,1].plot(np.unique(x_mt), np.poly1d(np.polyfit(x_mt, y_mt, 1))(np.unique(x_mt)), linestyle='--', color='k', alpha=0.2)\n",
    "    #plot the scatter points with error bars\n",
    "    axs[1,1].scatter(x_mt,y_mt, label='Data', marker='o', color='#8B0000', s=25, zorder=2, alpha=.5)\n",
    "    axs[1,1].errorbar(x_mt,y_mt, yerr=y_emt, linestyle=\"None\", color='k', alpha=.3, zorder=1)\n",
    "    #title and x,y labels\n",
    "    axs[1,1].set_title('Mud-Tank', fontsize=12)\n",
    "\n",
    "    ####OUTPUT(SAMPLES)\n",
    "    #plot the measured mean Uppm line in ref material\n",
    "    axs[0,0].plot(x_output,y_moutput, label='Mean', linestyle='-', color='#8B0000', alpha=0.5)\n",
    "    axs[0,0].text(np.mean(x_output), np.mean(y_moutput) + np.mean(y_moutput)*0.05,'mean: {0:.2f}'\" \"u'\\xb1'\" \"'{1:.2f}'\" \"'ppm' .format(round(y_moutput[0], 2),round(y_meoutput[0], 2)),fontsize=11, ha='center', color='#8B0000', zorder=3)\n",
    "    axs[0,0].plot(np.unique(x_output), np.poly1d(np.polyfit(x_output, y_output, 1))(np.unique(x_output)), linestyle='--', color='k', alpha=0.2)\n",
    "    #plot the scatter points with error bars\n",
    "    axs[0,0].scatter(x_output,y_output, label='Data', marker='o', color='#8B0000', s=25, zorder=2, alpha=.5)\n",
    "    axs[0,0].errorbar(x_output,y_output, yerr=y_eoutput, linestyle=\"None\", color='k', alpha=.3, zorder=1)\n",
    "    axs[0,0].set_yscale('log')\n",
    "    #title and x,y labels\n",
    "    axs[0,0].set_title('Sample: {}' .format(sample_name), fontsize=12)\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='time 10e-4 (s)', ylabel='[U] ppm')\n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    #for ax in axs.flat:\n",
    "    #    ax.label_outer()\n",
    "\n",
    "    #padding between sublpots\n",
    "    fig.tight_layout(pad=1.5)\n",
    "\n",
    "    #save the figure to file, location defined at the start\n",
    "    plt.savefig(\"{0}/{1}_Standards.pdf\".format(save_to, sample_name), bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################IMPORTANT CELL#####################################\n",
    "if counts_file_w.value != {} or counts_paths != []:\n",
    "    remove_grains_w = [widgets.Checkbox(True,description=g,layout=Layout(width='100%')) for g in age_df['Grain/Mica']]\n",
    "\n",
    "\n",
    "        \n",
    "    first_box = VBox([remove_grains_w[n] for n in range(0,int(round((age_df['Grain/Mica'].count()-1)/5)))])\n",
    "\n",
    "    second_box = VBox([remove_grains_w[n] for n in range(int(round((age_df['Grain/Mica'].count()-1)/5)),\n",
    "                                           int(round((age_df['Grain/Mica'].count()-1)/2.4)))])\n",
    "\n",
    "    third_box = VBox([remove_grains_w[n] for n in range(int(round((age_df['Grain/Mica'].count()-1)/2.4)),\n",
    "                                          int(round((age_df['Grain/Mica'].count()-1)/1.6)))])\n",
    "\n",
    "    fourth_box = VBox([remove_grains_w[n] for n in range(int(round((age_df['Grain/Mica'].count()-1)/1.6)),\n",
    "                                           int(round((age_df['Grain/Mica'].count()-1)/1.2)))])\n",
    "\n",
    "    fifth_box = VBox([remove_grains_w[n] for n in range(int(round((age_df['Grain/Mica'].count()-1)/1.2)),\n",
    "                                          age_df['Grain/Mica'].count())])\n",
    "        \n",
    "\n",
    "    print(\"REMOVE GRAINS FROM AGE EQUATION\\n   Included Grain IDs:\")\n",
    "    display(widgets.HBox([first_box, second_box, third_box, fourth_box, fifth_box]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6) CALCULATE/RE-CALCULATE THE AGE(S)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e139ee3b31b849aca57d1d5495168cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Calculate/Re-calculate Age', layout=Layout(width='50%'), style=But…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"6) CALCULATE/RE-CALCULATE THE AGE(S)\")\n",
    "def calc_age(b):\n",
    "    display(Javascript('IPython.notebook.execute_cells_below()'))\n",
    "calc_age_button = widgets.Button(description = \"Calculate/Re-calculate Age\",\n",
    "                                layout=Layout(width='50%'),button_style='success')\n",
    "\n",
    "calc_age_button.on_click(calc_age)\n",
    "\n",
    "display(calc_age_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRAINS WITH NOTES (for your review):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malcmcm/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/malcmcm/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >Grain/Mica</th>        <th class=\"col_heading level0 col1\" >Tracks</th>        <th class=\"col_heading level0 col2\" >Area(cm2)</th>        <th class=\"col_heading level0 col3\" >Density(tracks/cm2)</th>        <th class=\"col_heading level0 col4\" >Average DPar(µmm)</th>        <th class=\"col_heading level0 col5\" >U_ppm_m238</th>        <th class=\"col_heading level0 col6\" >Notes</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row0_col0\" class=\"data row0 col0\" >04</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row0_col1\" class=\"data row0 col1\" >133</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row0_col2\" class=\"data row0 col2\" >0.000045</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row0_col3\" class=\"data row0 col3\" >2.93E+06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row0_col4\" class=\"data row0 col4\" >1.350000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row0_col5\" class=\"data row0 col5\" >63.900000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row0_col6\" class=\"data row0 col6\" >zoned\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row1_col0\" class=\"data row1 col0\" >05</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row1_col1\" class=\"data row1 col1\" >61</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row1_col2\" class=\"data row1 col2\" >0.000028</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row1_col3\" class=\"data row1 col3\" >2.19E+06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row1_col4\" class=\"data row1 col4\" >1.360000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row1_col5\" class=\"data row1 col5\" >22.100000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row1_col6\" class=\"data row1 col6\" >zoned\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row2_col0\" class=\"data row2 col0\" >06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row2_col3\" class=\"data row2 col3\" >�</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row2_col4\" class=\"data row2 col4\" >nan</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row2_col5\" class=\"data row2 col5\" >2.650000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row2_col6\" class=\"data row2 col6\" >dpars all messed up</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row3_col0\" class=\"data row3 col0\" >07</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row3_col3\" class=\"data row3 col3\" >�</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row3_col4\" class=\"data row3 col4\" >nan</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row3_col5\" class=\"data row3 col5\" >5.390000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row3_col6\" class=\"data row3 col6\" >lots of inclusions</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row4_col0\" class=\"data row4 col0\" >13</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row4_col1\" class=\"data row4 col1\" >96</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row4_col2\" class=\"data row4 col2\" >0.000042</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row4_col3\" class=\"data row4 col3\" >2.29E+06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row4_col4\" class=\"data row4 col4\" >1.530000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row4_col5\" class=\"data row4 col5\" >44.500000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row4_col6\" class=\"data row4 col6\" >might have some inclusions</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row5_col0\" class=\"data row5 col0\" >15</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row5_col1\" class=\"data row5 col1\" >11</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row5_col2\" class=\"data row5 col2\" >0.000043</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row5_col3\" class=\"data row5 col3\" >2.54E+05</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row5_col4\" class=\"data row5 col4\" >1.170000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row5_col5\" class=\"data row5 col5\" >7.300000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row5_col6\" class=\"data row5 col6\" >some non track features</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row6_col0\" class=\"data row6 col0\" >20</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row6_col1\" class=\"data row6 col1\" >18</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row6_col2\" class=\"data row6 col2\" >0.000049</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row6_col3\" class=\"data row6 col3\" >3.66E+05</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row6_col4\" class=\"data row6 col4\" >1.310000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row6_col5\" class=\"data row6 col5\" >7.000000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row6_col6\" class=\"data row6 col6\" >dpars off a bit</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row7_col0\" class=\"data row7 col0\" >21</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row7_col1\" class=\"data row7 col1\" >99</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row7_col2\" class=\"data row7 col2\" >0.000059</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row7_col3\" class=\"data row7 col3\" >1.69E+06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row7_col4\" class=\"data row7 col4\" >1.400000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row7_col5\" class=\"data row7 col5\" >21.400000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row7_col6\" class=\"data row7 col6\" >alot of non-track features</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row8_col0\" class=\"data row8 col0\" >23</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row8_col1\" class=\"data row8 col1\" >102</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row8_col2\" class=\"data row8 col2\" >0.000053</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row8_col3\" class=\"data row8 col3\" >1.93E+06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row8_col4\" class=\"data row8 col4\" >1.440000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row8_col5\" class=\"data row8 col5\" >45.200000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row8_col6\" class=\"data row8 col6\" >zoned\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row9_col0\" class=\"data row9 col0\" >24</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row9_col1\" class=\"data row9 col1\" >98</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row9_col2\" class=\"data row9 col2\" >0.000040</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row9_col3\" class=\"data row9 col3\" >2.43E+06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row9_col4\" class=\"data row9 col4\" >1.520000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row9_col5\" class=\"data row9 col5\" >52.900000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row9_col6\" class=\"data row9 col6\" >zoned\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row10_col0\" class=\"data row10 col0\" >26</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row10_col1\" class=\"data row10 col1\" >71</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row10_col2\" class=\"data row10 col2\" >0.000037</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row10_col3\" class=\"data row10 col3\" >1.93E+06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row10_col4\" class=\"data row10 col4\" >1.520000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row10_col5\" class=\"data row10 col5\" >26.500000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row10_col6\" class=\"data row10 col6\" >zoned\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row11_col0\" class=\"data row11 col0\" >29</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row11_col1\" class=\"data row11 col1\" >31</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row11_col2\" class=\"data row11 col2\" >0.000069</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row11_col3\" class=\"data row11 col3\" >4.50E+05</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row11_col4\" class=\"data row11 col4\" >1.200000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row11_col5\" class=\"data row11 col5\" >7.500000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row11_col6\" class=\"data row11 col6\" >suspicous features</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row12_col0\" class=\"data row12 col0\" >32</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row12_col1\" class=\"data row12 col1\" >45</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row12_col2\" class=\"data row12 col2\" >0.000038</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row12_col3\" class=\"data row12 col3\" >1.17E+06</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row12_col4\" class=\"data row12 col4\" >1.340000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row12_col5\" class=\"data row12 col5\" >25.800000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row12_col6\" class=\"data row12 col6\" >zoned\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row13_col0\" class=\"data row13 col0\" >38</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row13_col1\" class=\"data row13 col1\" >45</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row13_col2\" class=\"data row13 col2\" >0.000058</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row13_col3\" class=\"data row13 col3\" >7.77E+05</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row13_col4\" class=\"data row13 col4\" >1.440000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row13_col5\" class=\"data row13 col5\" >16.600000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row13_col6\" class=\"data row13 col6\" >zoned?\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row14_col0\" class=\"data row14 col0\" >40</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row14_col1\" class=\"data row14 col1\" >30</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row14_col2\" class=\"data row14 col2\" >0.000033</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row14_col3\" class=\"data row14 col3\" >9.24E+05</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row14_col4\" class=\"data row14 col4\" >1.370000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row14_col5\" class=\"data row14 col5\" >10.600000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row14_col6\" class=\"data row14 col6\" >non-track features\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row15_col0\" class=\"data row15 col0\" >43</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row15_col1\" class=\"data row15 col1\" >0</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row15_col2\" class=\"data row15 col2\" >0.000007</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row15_col3\" class=\"data row15 col3\" >0.00E+00</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row15_col4\" class=\"data row15 col4\" >nan</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row15_col5\" class=\"data row15 col5\" >15.700000</td>\n",
       "                        <td id=\"T_08d8eb94_6db3_11ea_b6ab_acde48001122row15_col6\" class=\"data row15 col6\" >zoned?\n",
       "</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a183d2780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if counts_file_w.value != {} or counts_paths != []:\n",
    "    ####DEFINE VARIABLES IN THE DATAFRAM(S) USED TO CLACULATE AGES, ETC..####\n",
    "\n",
    "    ns = age_df['Tracks']\n",
    "    area = age_df['Area(cm2)']\n",
    "    ps = age_df['Tracks']/age_df['Area(cm2)']\n",
    "    U_ppm = age_df['U_ppm_m238']\n",
    "    U_ppm_1sigma = age_df['U_ppm_m238_Int2SE']\n",
    "    Th_ppm = age_df['Th_ppm_m232']\n",
    "    Th_ppm_1sigma = age_df['Th_ppm_m232_Int2SE']\n",
    "    Dpar = age_df['Average DPar(µmm)']\n",
    "    Dpar_mean = round(np.mean(Dpar),2)\n",
    "    Dpar_1sigma = age_df['DPar Std Deviation']\n",
    "    no_grains=ps.count()\n",
    "    grain_id=age_df['Grain/Mica']\n",
    "    eU = U_ppm+(0.245*Th_ppm)\n",
    "    eU_1sigma = U_ppm_1sigma+0.245*Th_ppm_1sigma\n",
    "    rmr0D =  round(0.84*((4.58-(0.9231*Dpar+0.2515))/2.98)**(0.21),4)\n",
    "\n",
    "        #checks if there is a column in the age_df dataframe named \"Cl\" (if you've added Cl data)\n",
    "    if 'Cl' in age_df.columns:\n",
    "        Cl_sem = round(age_df['Cl'],3)\n",
    "    else:\n",
    "        Cl_sem = '--'\n",
    "    if 'rmr0' in age_df.columns:\n",
    "        rmr0 = round(age_df['rmr0'],3)\n",
    "    else:\n",
    "        rmr0 = '--'\n",
    "\n",
    "\n",
    "\n",
    "    #Generate Single Grain Ages\n",
    "    ri = U_ppm/(area*((U_ppm_1sigma)**2))\n",
    "    ni = (U_ppm/U_ppm_1sigma)**2\n",
    "        #Takes care of zero track grains after Vermeesch, 2017\n",
    "    Age_sga = np.where(ns==0., 1/ld*np.log10(1+0.45*ld*ri*(ns+0.5)/((ni)+0.5))/100, (1/(ld)*np.log(1+Xi*ld*ps/U_ppm)))\n",
    "    Age_sga_1sigma = np.where(ns==0., Age_sga*((1/0.5+1/ni)**(0.5)), (1/(ld)*np.log(1+Xi*ld*ps/U_ppm))*(((1/ns)+(U_ppm_1sigma/U_ppm)**(2))**(0.5)))\n",
    "    ps = np.where(ns==0., ri, ps)\n",
    "\n",
    "    #Generate Pooled age\n",
    "    Pooled_age = (1/(ld)*np.log(1+Xi*ld*np.sum(ns)/np.sum(U_ppm*area)))\n",
    "        #pooled age error\n",
    "    f_Ti=1/(((2*np.pi)**(0.5))*Age_sga_1sigma/Age_sga)\n",
    "    f_Ti_k=f_Ti/np.sum(f_Ti)\n",
    "    Pooled_age_1sigma = (np.sum(((Age_sga-Pooled_age)**2)*f_Ti_k)**0.5)/(no_grains-1)**0.5\n",
    "\n",
    "    #chi-square test\n",
    "        #Calculates X2 after Vermeesch, 2017 (same as radial plotter X2)\n",
    "    zj = np.log(Age_sga)\n",
    "    sj = Age_sga_1sigma/Age_sga\n",
    "    X2 = np.sum((zj/sj)**2)-((np.sum(zj/(sj**2)))**2)/(np.sum(1/(sj**2))) #X2 from Vermeesch 2017\n",
    "    PX2_perc = stats.chi2.sf(X2, no_grains-1)*100\n",
    "\n",
    "    #central age and dispersion is calculated later\n",
    "\n",
    "        #create a new dataframe with the ages etc.. (sorry for naming standard deviation 4 diff things, cant use same object twice)\n",
    "    ns_df = pd.DataFrame({'Grain ID': grain_id, 'Ns': ns, 'Area (cm2)': area,\n",
    "                          'ps (cm2)': ps, '238U (ppm)': U_ppm, '±':'±','1SD(U)': U_ppm_1sigma, \n",
    "                           'eU (ppm)': eU,'±2':'±', '1SD(eU)': eU_1sigma, 'Age(Ma)': Age_sga,\n",
    "                          '±3':'±','1SD(Age)': Age_sga_1sigma,'Dpar (µm)': Dpar,\n",
    "                          '±4':'±','1SD(Dp)': Dpar_1sigma, 'Cl wt (%)': Cl_sem, 'rmr0': rmr0, 'rmr0D': rmr0D})\n",
    "\n",
    "    ns_df.reset_index(drop=True, inplace=True)\n",
    "    #ns_df\n",
    "    ###SEE ONLY THOSE GRAINS THAT HAVE SOMETHING IN THE NOTES COLUMN####\n",
    "    grain_notes = age_df[age_df['Notes'].notnull()]\n",
    "    grain_notes = grain_notes[[\"Grain/Mica\", \"Tracks\", \"Area(cm2)\", \"Density(tracks/cm2)\", 'Average DPar(µmm)', \"U_ppm_m238\", \"Notes\",]]\n",
    "    print(\"GRAINS WITH NOTES (for your review):\")\n",
    "    display(grain_notes.style.hide_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRAINS REMOVED FROM AGE EQUATION:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_132261a2_6db3_11ea_8863_acde48001122\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >Grain/Mica</th>        <th class=\"col_heading level0 col1\" >Reason for removal</th>        <th class=\"col_heading level0 col2\" >Tracks</th>        <th class=\"col_heading level0 col3\" >U_ppm_m238</th>        <th class=\"col_heading level0 col4\" >Notes</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row0_col0\" class=\"data row0 col0\" >01</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row0_col1\" class=\"data row0 col1\" >User Removed</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row0_col2\" class=\"data row0 col2\" >10</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row0_col3\" class=\"data row0 col3\" >7.410000</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row1_col0\" class=\"data row1 col0\" >02</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row1_col1\" class=\"data row1 col1\" >User Removed</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row1_col2\" class=\"data row1 col2\" >11</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row1_col3\" class=\"data row1 col3\" >4.840000</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row1_col4\" class=\"data row1 col4\" >nan</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row2_col0\" class=\"data row2 col0\" >03</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row2_col1\" class=\"data row2 col1\" >User Removed</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row2_col2\" class=\"data row2 col2\" >15</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row2_col3\" class=\"data row2 col3\" >2.430000</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row2_col4\" class=\"data row2 col4\" >nan</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row3_col0\" class=\"data row3 col0\" >06</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row3_col1\" class=\"data row3 col1\" >Did not measure</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row3_col3\" class=\"data row3 col3\" >2.650000</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row3_col4\" class=\"data row3 col4\" >dpars all messed up</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row4_col0\" class=\"data row4 col0\" >07</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row4_col1\" class=\"data row4 col1\" >Did not measure</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row4_col3\" class=\"data row4 col3\" >5.390000</td>\n",
       "                        <td id=\"T_132261a2_6db3_11ea_8863_acde48001122row4_col4\" class=\"data row4 col4\" >lots of inclusions</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a182583c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if counts_file_w.value != {} or counts_paths != []:\n",
    "    remove_grs = []\n",
    "    for i in range(0,age_df['Grain/Mica'].count()-1):\n",
    "        if remove_grains_w[i].value == False:\n",
    "            remove_grs.append(remove_grains_w[i].description)\n",
    "    ns_df = ns_df[~ns_df['Grain ID'].isin(remove_grs)]\n",
    "        #removes grains with [U] = 0, aka \"Below LOD\" that give an age of 'infinity' and break my code :(\n",
    "    ns_df = ns_df[ns_df['238U (ppm)'] != 0.]\n",
    "        #removes grains that haven't been counted in FastTracks based on no measured area\n",
    "    ns_df = ns_df[ns_df['Area (cm2)'] != 0.]\n",
    "\n",
    "    #adds removed grains from above to new list: ns_df_removed\n",
    "    ns_df_removed = age_df[age_df['Grain/Mica'].isin(remove_grs)]\n",
    "    ns_df_removed = ns_df_removed.append(age_df[age_df['Area(cm2)'] == 0])\n",
    "    ns_df_removed = ns_df_removed.append(age_df[age_df['U_ppm_m238'] == 0])\n",
    "    ns_df_removed.drop_duplicates(subset =\"Grain/Mica\",keep = 'first', inplace = True)\n",
    "        #notes reason for removal in column\n",
    "    ns_df_removed.insert(1, 'Reason for removal', 'User Removed')\n",
    "    ns_df_removed.loc[ns_df_removed['U_ppm_m238'] == 0, 'Reason for removal'] = \"[U] Below LOD\"\n",
    "    ns_df_removed.loc[ns_df_removed['Area(cm2)'] == 0, 'Reason for removal'] = \"Did not measure\"\n",
    "        #select column headers we want; remove unwanted columns\n",
    "    ns_df_removed = ns_df_removed[['Grain/Mica','Reason for removal','Tracks','U_ppm_m238','Notes']]\n",
    "\n",
    "        #inspect the grains you've removed, make sure you've removed the correct grains\n",
    "    print(\"GRAINS REMOVED FROM AGE EQUATION:\")\n",
    "    display(ns_df_removed.style.hide_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled Age: 114.22 ± 9.67 Ma\n",
      "Central Age: 121.85 ± 7.53 Ma\n",
      "No. of grains:  39\n",
      "---------\n",
      "X2: 153.49\n",
      "P(X2): 0.0 %\n",
      "Dispersion: 31.03 %\n"
     ]
    }
   ],
   "source": [
    "if counts_file_w.value != {} or counts_paths != []: \n",
    "    ####RE-CALCULATE NEW AGES BASED ON THE REMOVAL OF GRAINS, FINALLY CALCULATES THE CENTRAL AGE####\n",
    "\n",
    "    ns = ns_df['Ns']\n",
    "    area = ns_df['Area (cm2)']\n",
    "    ps = ns_df['Ns']/ns_df['Area (cm2)']\n",
    "    U_ppm = ns_df['238U (ppm)']\n",
    "    U_ppm_1sigma = ns_df['1SD(U)']\n",
    "    Dpar = ns_df['Dpar (µm)']\n",
    "    Dpar_mean = round(np.mean(Dpar),2)\n",
    "    Dpar_1sigma = ns_df['1SD(Dp)']\n",
    "    no_grains=ps.count()\n",
    "    grain_id=ns_df['Grain ID']\n",
    "    eU = ns_df['eU (ppm)']\n",
    "    eU_1sigma = ns_df['1SD(eU)']\n",
    "    Cl_sem = ns_df['Cl wt (%)']\n",
    "    rmr0 = ns_df['rmr0']\n",
    "    rmr0D = ns_df['rmr0D']\n",
    "\n",
    "        #checks if there is a column in the age_df dataframe named \"Cl\" (if you've added Cl data)\n",
    "    if 'Cl' in age_df.columns:\n",
    "        Cl_sem_mean = round(np.mean(Cl_sem),3)\n",
    "        Cl_sem_sdm = round(np.std(Cl_sem), 3)\n",
    "    else:\n",
    "        Cl_sem_mean = '--'\n",
    "        Cl_sem_sdm = '--'\n",
    "    if 'rmr0' in age_df.columns:\n",
    "        rmr0_mean = round(np.mean(rmr0),3)\n",
    "        rmr0_sdm = round(np.std(rmr0), 3)\n",
    "    else:\n",
    "        rmr0_mean = '--'\n",
    "        rmr0_sdm = '--'\n",
    "\n",
    "    rmr0D_mean = round(np.mean(rmr0D),3)\n",
    "    rmr0D_sdm = round(np.std(rmr0D), 3)\n",
    "\n",
    "    #Generate Single Grain Ages\n",
    "    ri = U_ppm/(area*((U_ppm_1sigma)**2))\n",
    "    ni = (U_ppm/U_ppm_1sigma)**2\n",
    "        #Handles zero track grains after Vermeesch, 2017\n",
    "    Age_sga = np.where(ns==0., 1/ld*np.log10(1+0.45*ld*ri*(ns+0.5)/((ni)+0.5))/100, (1/(ld)*np.log(1+Xi*ld*ps/U_ppm)))\n",
    "    Age_sga_1sigma = np.where(ns==0., Age_sga*((1/0.5+1/ni)**(0.5)), (1/(ld)*np.log(1+Xi*ld*ps/U_ppm))*(((1/ns)+(U_ppm_1sigma/U_ppm)**(2))**(0.5)))\n",
    "    ps = np.where(ns==0., ri, ps)\n",
    "\n",
    "    #Pooled age\n",
    "    Pooled_age = (1/(ld)*np.log(1+Xi*ld*np.sum(ns)/np.sum(U_ppm*area)))\n",
    "    #pooled age error\n",
    "    f_Ti=1/(((2*np.pi)**(0.5))*Age_sga_1sigma/Age_sga)\n",
    "    f_Ti_k=f_Ti/np.sum(f_Ti)\n",
    "    Pooled_age_1sigma = (np.sum(((Age_sga-Pooled_age)**2)*f_Ti_k)**0.5)/(no_grains-1)**0.5\n",
    "\n",
    "\n",
    "    #chi-square test\n",
    "        #Calculates X2 after Vermeesch, 2017 (same as radial plotter X2)\n",
    "    zj = np.log(Age_sga)\n",
    "    sj = Age_sga_1sigma/Age_sga\n",
    "    X2 = np.sum((zj/sj)**2)-((np.sum(zj/(sj**2)))**2)/(np.sum(1/(sj**2))) #X2 from Vermeesch 2017\n",
    "    PX2_perc = stats.chi2.sf(X2, no_grains-1)*100\n",
    "\n",
    "    ns_df = pd.DataFrame({'Grain ID': grain_id, 'Ns': ns, 'Area (cm2)': area,\n",
    "                          'ps (cm2)': ps, '238U (ppm)': U_ppm, '±':'±','1SD(U)': U_ppm_1sigma, \n",
    "                           'eU (ppm)': eU,'±2':'±', '1SD(eU)': eU_1sigma, 'Age(Ma)': Age_sga,\n",
    "                          '±3':'±','1SD(Age)': Age_sga_1sigma,'Dpar (µm)': Dpar,\n",
    "                          '±4':'±','1SD(Dp)': Dpar_1sigma, 'Cl wt (%)': Cl_sem, 'rmr0': rmr0, 'rmr0D': rmr0D})\n",
    "\n",
    "    ns_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Central age and dispersion\n",
    "        #uses the CentralAge.py function, should exist in the same folder as this py project.\n",
    "        #expects 4 arguments CentralAge(grain_ids, single_grain_ages, single_grain_1sigma, pooled_age)\n",
    "        #grain ids should be something that it can get the total grain count\n",
    "    C_A = CentralAge(ns_df['Grain ID'], ns_df['Age(Ma)'], ns_df['1SD(Age)'], Pooled_age)\n",
    "        #returns 3 variables, in this order: Central Age, Central Age 1sigma, Dispersion\n",
    "    Central_age = C_A[0]\n",
    "    Central_age_1sigma = C_A[1]\n",
    "    Dispersion = C_A[2]\n",
    "\n",
    "    print('Pooled Age:', round(Pooled_age, 2), '±', round(Pooled_age_1sigma,2),'Ma')\n",
    "    print('Central Age:', round(Central_age, 2), '±', round(Central_age_1sigma, 2),'Ma')\n",
    "    print('No. of grains: ',grain_id.count())\n",
    "    print('---------')\n",
    "    print('X2:', round(X2, 2))\n",
    "    print('P(X2):', round(PX2_perc, 2),'%')\n",
    "    print('Dispersion:', round(Dispersion,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counts_file_w.value != {} or counts_paths != []:\n",
    "    ns_df_sorted = ns_df.sort_values(by='Age(Ma)')\n",
    "    ns_df_sorted.reset_index(drop=True, inplace=True)\n",
    "    Age_sga_sorted = ns_df_sorted['Age(Ma)']\n",
    "    Age_sga_1sigma_sorted = ns_df_sorted['1SD(Age)']\n",
    "\n",
    "    U_ppm_sorted = ns_df_sorted['238U (ppm)']\n",
    "    U_ppm_1sigma_sorted = ns_df_sorted['1SD(U)']\n",
    "    Dpar_sorted = ns_df_sorted['Dpar (µm)']\n",
    "    Dpar_1sigma_sorted = ns_df_sorted['1SD(Dp)']\n",
    "    no_grains_sorted=Age_sga_sorted.count()\n",
    "    grain_id_sorted=ns_df_sorted['Grain ID']\n",
    "    eU_sorted = ns_df_sorted['eU (ppm)']\n",
    "    eU_1sigma_sorted = ns_df_sorted['1SD(eU)']\n",
    "\n",
    "    ages_enumerated = []\n",
    "    for i, ages in enumerate(Age_sga_sorted):\n",
    "        ages_e = ages/ages+i\n",
    "        ages_enumerated.append(ages_e)\n",
    "    U_enumerated = []\n",
    "    for i, U in enumerate(U_ppm_sorted):\n",
    "        U_e = U/U+i\n",
    "        U_enumerated.append(U_e)\n",
    "\n",
    "    class MidpointNormalize(mpl.colors.Normalize):\n",
    "        def __init__(self, vmin=None, vmax=None, vcenter=None, clip=False):\n",
    "            self.vcenter = vcenter\n",
    "            mpl.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "        def __call__(self, value, clip=None):\n",
    "            # I'm ignoring masked values and all kinds of edge cases to make a\n",
    "            # simple example...\n",
    "            x, y = [self.vmin, self.vcenter, self.vmax], [0, 0.5, 1]\n",
    "            return np.ma.masked_array(np.interp(value, x, y))\n",
    "    midnorm = MidpointNormalize(vmin=min(U_ppm_sorted), vcenter=np.mean(U_ppm_sorted), vmax=max(U_ppm_sorted))\n",
    "\n",
    "    #lay out the figures using GridSpec\n",
    "    fig1 = plt.figure(constrained_layout=True, figsize=(16,7),)\n",
    "    spec1 = gs.GridSpec(ncols=3, nrows=1, figure=fig1, width_ratios=(10,0.5,10))\n",
    "\n",
    "        #left figure\n",
    "    ax = fig1.add_subplot(spec1[0, 0])\n",
    "        #plot the scatter points with error bars\n",
    "    Ucolor = ax.scatter(ages_enumerated,Age_sga_sorted, marker='s', c=U_ppm_sorted, cmap='coolwarm',\n",
    "               s=140, alpha=0.6, norm=midnorm, zorder=2, label='_nolegend_')\n",
    "    ax.errorbar(ages_enumerated,Age_sga_sorted, yerr=Age_sga_1sigma_sorted,\n",
    "                linestyle=\"None\", c='k', alpha=0.2, linewidth=2.5,zorder=1, label='_nolegend_')\n",
    "        #annotate each plot point with grain id\n",
    "    for i, txt in enumerate(grain_id_sorted):\n",
    "        ax.annotate(txt, (ages_enumerated[i], Age_sga_sorted[i]), fontsize=9, weight = 'bold',\n",
    "                    ha='center', va='center', color='k',alpha=0.8,zorder=3)\n",
    "\n",
    "    ax.set_yscale('linear') #change to log if you wish\n",
    "    ax.autoscale(enable=True, axis='y', tight=True)\n",
    "        #color span for pooled & central age ranges\n",
    "    ax.axhspan(Pooled_age-Pooled_age_1sigma, Pooled_age+Pooled_age_1sigma, alpha=0.2,\n",
    "               edgecolor = 'k',facecolor='none',linestyle='--',linewidth=1.5, label='P. Age±SD', zorder=4)\n",
    "    ax.axhspan(Central_age-Central_age_1sigma, Central_age+Central_age_1sigma, alpha=0.2,\n",
    "               color='cornflowerblue', label='C. Age±SD', zorder=5)\n",
    "        #titles and other text\n",
    "    ax.legend(loc='upper right')\n",
    "    if ns_df_removed['Grain/Mica'].empty:\n",
    "        ax.text(0.01, 0.99, 'Pooled Age = {0:.1f} ± {1:.1f}Ma\\nCentral Age = {2:.1f} ± {3:.1f}Ma\\n χ2 = {4:.1f}\\n p(χ2) = {5:.1f}%\\n Disp. = {6:.1f}%\\n\\n Total Grains = {7}'.format(Pooled_age, Pooled_age_1sigma, Central_age, Central_age_1sigma, X2, PX2_perc, Dispersion, grain_id.count()),\n",
    "                style='italic', horizontalalignment='left', verticalalignment='top', transform=ax.transAxes, fontsize=11)\n",
    "    else:\n",
    "        ax.text(0.01, 0.99, 'Pooled Age = {0:.1f} ± {1:.1f}Ma\\nCentral Age = {2:.1f} ± {3:.1f}Ma\\n χ2 = {4:.1f}\\n p(χ2) = {5:.1f}%\\n Disp. = {6:.1f}%\\n\\n Total Grains = {7}\\n Removed GrainID(s):\\n{8}'.format(Pooled_age, Pooled_age_1sigma, Central_age, Central_age_1sigma, X2, PX2_perc, Dispersion, grain_id.count(),\n",
    "        ns_df_removed['Grain/Mica'].to_string(index=False)), style='italic', horizontalalignment='left', verticalalignment='top', transform=ax.transAxes, fontsize=11)\n",
    "    ax.set_title('{0} Single Grain Ages' .format(sample_name))\n",
    "    ax.set_ylabel('Age (Ma)', fontsize=12)\n",
    "    ax.set_xlabel('Ascending count based on age', fontsize=12)\n",
    "\n",
    "    #axes = plt.subplot(gs[0,1])\n",
    "    ax3 = plt.subplot(spec1[0, 1])\n",
    "    c_bar = plt.colorbar(Ucolor, cax=ax3)\n",
    "    c_bar.set_label(\"[U]ppm\", fontsize=10)\n",
    "\n",
    "    ##### right figure\n",
    "    ax2 = fig1.add_subplot(spec1[0, 2])\n",
    "    uplot = ax2.scatter(x=U_enumerated,y=U_ppm_sorted,marker='o', color='white', s=100, alpha=1, zorder=2)\n",
    "    ax2.errorbar(x=U_enumerated,y=U_ppm_sorted, yerr=U_ppm_1sigma_sorted, linestyle=\"None\", linewidth=2.5,color='k', alpha=0.2, zorder=1)\n",
    "    ax2.grid(which='major', axis='y', alpha=0.6)    \n",
    "    ax2.grid(which='minor', axis='y', alpha=0.2)   \n",
    "        #annotate each plot point with grain number\n",
    "    for i, txt in enumerate(grain_id_sorted):\n",
    "        ax2.annotate(txt, (U_enumerated[i], U_ppm_sorted[i]), fontsize=9, weight = 'bold', ha='center', va='center', color='#942222', zorder=3)\n",
    "    #for i,j in zip(grain_id,U_ppm):\n",
    "    #    ax2.annotate(str(i),xy=(i,j), fontsize=9, weight = 'bold', ha='center', va='center', color='#90033C', zorder=3)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    ax2.yaxis.tick_right()\n",
    "        #color span for u±1sigma\n",
    "    ax2.axhspan(np.mean(U_ppm)-np.mean(U_ppm_1sigma), np.mean(U_ppm)+np.mean(U_ppm_1sigma), alpha=0.2, color='cornflowerblue')\n",
    "        #draw line at less than 1 ppm, consider removing these grains (anonymously old usualy)\n",
    "    ax2.axhline(y=1,linewidth=1, linestyle='--', color='#970000')\n",
    "    ax2.axhspan(1, 0, alpha=0.1, color='r')\n",
    "        #plot text that appears inside the plot\n",
    "    ax2.text(0, np.mean(U_ppm)-np.mean(U_ppm_1sigma)+0.1, 'mean [U]: {0:.2f}±{1:.1f}ppm'.format(np.mean(U_ppm),np.mean(U_ppm_1sigma)) , style='italic', horizontalalignment='left', fontsize=9, color='cornflowerblue')\n",
    "\n",
    "\n",
    "        #titles and other text\n",
    "    #ax2.legend(loc='upper right')\n",
    "    ax2.set_title('{0} [U] ppm' .format(sample_name))\n",
    "    ax2.set_ylabel('[U] ppm', fontsize=12)\n",
    "    ax2.set_xlabel('Ascending count based on age', fontsize=12)\n",
    "\n",
    "        #save the figure to file, location defined at the start\n",
    "    plt.savefig(\"{0}/{1}_GrainvAge_U.pdf\".format(save_to, sample_name), bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counts_file_w.value != {} or counts_paths != []:\n",
    "    #Finally, save and export summary table and other sheets to import into modelling prog.\n",
    "\n",
    "    #Create summary table to single grain ages:\n",
    "        #header\n",
    "    single_grain_header = pd.DataFrame({'col0':['Sample:', 'Mineral:', 'Rock Type:', 'Lat/Long:', 'Elevation:', '','Grain ID'],\n",
    "                                       'col1':['', '', '', '', '','','Ns'],\n",
    "                                       'col2':[sample_name,mineral,rock_type,latitude,'{}m'.format(elevation),'','Area (cm2)'],\n",
    "                                       'col3':['', '', '','/ {}'.format(longitude), '','','ρS (cm-2)'],\n",
    "                                       'col4':['', '', '', '', '','','[U] (ppm)'],\n",
    "                                       'col5':['', '', '', '', '','','±'],\n",
    "                                       'col6':['', '', '', '', '','','1σ'],\n",
    "                                       'col7':['', '', '', '', '','','e[U] (ppm)'],\n",
    "                                       'col8':['', '', '', '', '','','±'],\n",
    "                                       'col9':['', '', '', '', '','','1σ'],\n",
    "                                       'col10':['', '', '', '', '','','Age (Ma)'],\n",
    "                                       'col11':['', '', '', '', '','','±'],\n",
    "                                       'col12':['', '', '', '', '','','1σ'],\n",
    "                                       'col13':['Analyst:', 'X/Y:', 'Date:', 'Software:', 'U Stand.:','','Dpar (µm)'],\n",
    "                                       'col14':['', '', '', '', '','','±'],\n",
    "                                       'col15':[analyst,'{0:.5f}/{1:.5f} (µm/px)'.format(px,py),date_meas.strftime(\"%d/%m/%Y\"),software,Ustandard,'','SE'],\n",
    "                                       'col16':['', '', '', '', '','','‡rmr0'],\n",
    "                                       'col17':['', '', '', '', '','','Cl wt%'],\n",
    "                                       'col18':['', '', '', '', '','','rmr0']})\n",
    "\n",
    "        #body of single grain ages\n",
    "    single_grain_body = pd.DataFrame({'col0':grain_id,\n",
    "                                        'col1':ns.astype(int),\n",
    "                                        'col2':area,\n",
    "                                        'col3':ps.round(2),\n",
    "                                        'col4':U_ppm.round(2),\n",
    "                                        'col5':'±',\n",
    "                                        'col6':U_ppm_1sigma.round(2),\n",
    "                                        'col7':eU.round(2),\n",
    "                                        'col8':'±',\n",
    "                                        'col9':eU_1sigma.round(2),\n",
    "                                        'col10':Age_sga.round(2),\n",
    "                                        'col11':'±',\n",
    "                                        'col12':Age_sga_1sigma.round(2),\n",
    "                                        'col13':Dpar.round(2),\n",
    "                                        'col14':'±',\n",
    "                                        'col15':Dpar_1sigma.round(2),\n",
    "                                        'col16':rmr0D,\n",
    "                                        'col17':Cl_sem,\n",
    "                                        'col18':rmr0,})\n",
    "    single_grain_body = single_grain_body.replace({\"col3\":{0:np.nan}})\n",
    "    single_grain_body = single_grain_body.replace({\"col15\":{0:np.nan}})\n",
    "\n",
    "        #footer of single grain ages\n",
    "    single_grain_footer = pd.DataFrame({'col0':['',ns.count(), '†SD of mean, ‡rmr0 from Dpar (Ketcham et al., 2007)', '', 'χ2', 'p(χ2)', 'Dispersion'],\n",
    "                                       'col1':['',np.sum(ns).astype(int), '', '', '=', '=', '='],\n",
    "                                       'col2':['',\"{:.3E}\".format(np.sum(area)), '', '',\"{:.2f}\".format(X2), \"{:.0f}%\".format(PX2_perc),\"{:.0f}%\".format(Dispersion)],\n",
    "                                       'col3':['',\"{:.3E}\".format(np.mean(ps)), '', '', '', '', ''],\n",
    "                                       'col4':['',\"{:.2f}\".format(np.mean(U_ppm)), '', '', '', '', ''],\n",
    "                                       'col5':['','±', '', '', '', '', ''],\n",
    "                                       'col6':['',\"{:.2f}†\".format(np.std(U_ppm)), '', '', '', '', ''],\n",
    "                                       'col7':['',\"{:.2f}\".format(np.mean(eU)), '', '', '', '', ''],\n",
    "                                       'col8':['','±', '', '', '', 'Pooled Age =', 'Central Age ='],\n",
    "                                       'col9':['',\"{:.2f}†\".format(np.std(eU)), '', '', '', '', ''],\n",
    "                                       'col10':['','', '', '', '',  \"{:.2f}\".format(Pooled_age),\"{:.2f}\".format(Central_age)],\n",
    "                                       'col11':['','', '', '', '', '±', '±'],\n",
    "                                       'col12':['','','', '', '', \"{:.2f}\".format(Pooled_age_1sigma), \"{:.2f}\".format(Central_age_1sigma)],\n",
    "                                       'col13':['',\"{:.2f}\".format(np.mean(Dpar)), '', '', '','',''],\n",
    "                                       'col14':['','±', '', '', '', '',''],\n",
    "                                       'col15':['',\"{:.2f}†\".format(np.std(Dpar)), '', '', '','',''],\n",
    "                                       'col16':['',rmr0D_mean, '', '', '', '', ''],\n",
    "                                       'col17':['',Cl_sem_mean, '', '', '', '', ''],\n",
    "                                       'col18':['',rmr0_mean, '', '', '', '', '']})\n",
    "\n",
    "    #bin the length data\n",
    "    length_range=[0,1,2,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
    "    if lengths_file_w.value != {}:\n",
    "        lengths_summary = lengths_data['True Length'].groupby(pd.cut(lengths_data['True Length'], length_range)).count()\n",
    "        lengths_binned = pd.DataFrame ({\"L1\":['','','','','','','Length (µm)','0-1','1-2','2-3','3-4','4-5','5-6',\n",
    "                        '6-7','7-8','8-9','9-10','10-11','11-12',\n",
    "                        '12-13','13-14','14-15','15-16','16-17','','MTL (µm) =','Var =','SD =','No. =','','Dpar (avg) =','‡rmr0 ='],\n",
    "                        \"L2\":['','','','','','','Count',lengths_summary[1],lengths_summary[2],lengths_summary[3],\n",
    "                        lengths_summary[4],lengths_summary[5],lengths_summary[6],lengths_summary[7],lengths_summary[8],\n",
    "                        lengths_summary[9],lengths_summary[10],lengths_summary[11],lengths_summary[12],lengths_summary[13],\n",
    "                        lengths_summary[14],lengths_summary[15],lengths_summary[16],lengths_summary[17],\n",
    "                        '',\"{:.2f}\".format(mtl),\n",
    "                        \"{:.2f}\".format(mtl_var)\n",
    "                        ,\"{:.2f}\".format(mtl_sd),l_no,'',Dpar_lengths_mean,rmr0D_lengths_mean]}) \n",
    "    else:\n",
    "        lengths_binned = pd.DataFrame ({\"L1\":[''],\"L2\":['']})\n",
    "\n",
    "    #combine the above dataframes horizontally (stack)\n",
    "    single_grain_data = pd.concat([single_grain_header, single_grain_body, single_grain_footer], axis=0)\n",
    "    #add the length bins to the right side\n",
    "    single_grain_data = pd.concat([single_grain_data.reset_index(drop=1),lengths_binned.reset_index(drop=1)], axis=1).fillna('')\n",
    "\n",
    "    #export a csv of summary body only data for possible later use\n",
    "    single_grain_sim = single_grain_body\n",
    "    single_grain_sim = single_grain_sim[[\"col0\",\"col1\",\"col2\",\"col3\",\"col4\",\"col6\",\"col7\",\"col9\",\"col10\",\"col12\",\"col13\",\"col15\",\"col16\",\"col17\",\"col18\"]]\n",
    "    single_grain_sim.columns = [\"grain\",\"ns\",\"area\",\"ps\",\"uppm\",\"uppm_sd\",\"euppm\",\"euppm_sd\",\"age\",\"age_sd\",\"dpar\",\"dpar_se\",\"rmr0D\",\"cl_wt\",\"rmr0\"]\n",
    "    single_grain_sim = pd.DataFrame(single_grain_sim)\n",
    "    single_grain_sim.reset_index(drop=True, inplace=True)\n",
    "    single_grain_sim.loc[0,'sample_no'] = sample_name \n",
    "    single_grain_sim.loc[0,'central_age'] = Central_age \n",
    "    single_grain_sim.loc[0,'central_age_sd'] = Central_age_1sigma\n",
    "    single_grain_sim.loc[0,'pooled_age'] = Pooled_age \n",
    "    single_grain_sim.loc[0,'pooled_age_sd'] = Pooled_age_1sigma\n",
    "    single_grain_sim.loc[0,'chi_2'] = X2\n",
    "    single_grain_sim.loc[0,'pchi'] = PX2_perc \n",
    "    single_grain_sim.loc[0,'dispersion'] = Dispersion\n",
    "    single_grain_sim.to_csv('{0}/{1}_age_summary.csv'.format(save_to, sample_name), index=None)\n",
    "\n",
    "    #export a csv of summary of lengths data for possible later use\n",
    "    if lengths_file_w.value != {}:\n",
    "        lengths_sim = lengths_data[[\"Length Name\", \"True Length\"]]\n",
    "        lengths_sim.columns = [\"length_no\",\"true_length\"]\n",
    "        lengths_sim = pd.DataFrame(lengths_sim)\n",
    "        lengths_sim.reset_index(drop=True, inplace=True)\n",
    "        lengths_sim['dpar_avg'] = Dpar_lengths\n",
    "        lengths_sim['rmr0'] = rmr0D_lengths\n",
    "        lengths_sim.to_csv('{0}/{1}_lengths_summary.csv'.format(save_to, sample_name), index=None)\n",
    "    ###############################################################################################################\n",
    "    #Create variables table\n",
    "    variables_table = pd.DataFrame({'Variable':[\"M[238U]\",\"N₀\",\"d(Ap)\",\"R(Ap)\",\"q(Ap)\",\"λd\",\"λf\",\"____\",\"ξ\"],\n",
    "                                   'Value':[M238U,No,dAp,\"{:.3e}\".format(RAp),qAp,ld,lf,\"\",\"{:.4e}\".format(Xi)],\n",
    "                                    'Unit':[\"g/mol\",\"—\",\"g/cm^3\",\"g/cm^3\",\"—\",\"1/t\",\"1/t\",\"\",\"tcm^2\"],\n",
    "                                    'Comment':[\"Molar Mass 238U\",\"Avogadros No.\",\n",
    "                                              \"Density of apatite. derived from a calculated relationship between density and apatite Cl content based on the analyses and unit cell dimensions of Carlson et al. (1999)\",\n",
    "                                              \"Based on half the confined track length for spontaneous tracks apatites (7.5 for volcanic apatites, see (Gleadow et al., 1986); 7.17 average of Durango).\",\n",
    "                                              \"The relatively few direct measurements of this efficiency factor range from 0.90–0.99 (e.g. Iwano et al., 1993; Jonckheere and Van den haute, 2002). Hasebe et al. (2004) used a value of 1.0. Clearly more experimental work is needed to define this parameter with the counting setup used, but for this discussion a value of 0.96 is used.\",\n",
    "                                              \"λd\",\n",
    "                                              \"Spontaneous fission decay constant, value from Yoshioka et al. (2005)\",\n",
    "                                              \"\",\"Aggregate factor = M238U/(λf*N₀*dAp*RAp*qAp)\"]})\n",
    "\n",
    "    ###########################################################\n",
    "    #Combine all above into 1 xlsx file with multiple sheets \n",
    "    writer = pd.ExcelWriter('{0}/{1}_Summary.xlsx'.format(save_to, sample_name),  \n",
    "                              engine ='xlsxwriter',options={'strings_to_urls': False, \n",
    "                                     'strings_to_formulas': False})\n",
    "    single_grain_data.to_excel(writer, sheet_name ='Ages', header = False, index = False)\n",
    "    #ns_df_removed.loc[no_grains+100,'U_ppm_m238'] = \"Copy/paste to retrieve:\"\n",
    "    #ns_df_removed.loc[no_grains+100,'Notes'] = str(remove_grs)\n",
    "    ns_df_removed.to_excel(writer, sheet_name ='Removed_Grains', header = True, index = False)\n",
    "    variables_table.to_excel(writer, sheet_name ='Variables', header = True, index = False)\n",
    "    single_grain_sim.to_excel(writer, sheet_name ='Single_Grain_Body', header = True, index = False)\n",
    "    counts_data.to_excel(writer, sheet_name ='Raw_Count_Data', header = True, index = False)\n",
    "    icpms_data.to_excel(writer, sheet_name ='Raw_ICPMS_Data', header = True, index = False)\n",
    "    if lengths_file_w.value != {}:\n",
    "        lengths_data.to_excel(writer, sheet_name ='Raw_Length_Data', header = True, index = False)\n",
    "\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Ages'] \n",
    "\n",
    "    sci_format = workbook.add_format({'num_format': '0.000E+00'})\n",
    "    format_boldC = workbook.add_format({'bold':  True, 'align': 'center'})\n",
    "    format_boldCTR = workbook.add_format({'bold':  True, 'align': 'center','text_wrap': True})\n",
    "    format_boldL = workbook.add_format({'bold':  True, 'align': 'left'})\n",
    "    format_center = workbook.add_format({'align': 'center'})\n",
    "    format_right = workbook.add_format({'align': 'right'})\n",
    "    format_left = workbook.add_format({'align': 'left'})\n",
    "    format_wrap = workbook.add_format({'align': 'left', 'valign': 'top', 'text_wrap': True})\n",
    "        #modify columns\n",
    "    worksheet.set_column('C:D', cell_format=sci_format)\n",
    "    worksheet.set_column('F:F', 2, cell_format=format_center)\n",
    "    worksheet.set_column('I:I', 2, cell_format=format_center)\n",
    "    worksheet.set_column('L:L', 2, cell_format=format_center)\n",
    "    worksheet.set_column('O:O', 2, cell_format=format_center)\n",
    "    worksheet.set_column('Q:Q', 6, cell_format=format_center)\n",
    "    worksheet.set_column('R:R', 6, cell_format=format_center)\n",
    "    worksheet.set_column('S:S', 6, cell_format=format_center)\n",
    "    worksheet.set_column('T:T', 10, cell_format=format_center)\n",
    "    worksheet.set_column('U:U', 8, cell_format=format_center)\n",
    "    worksheet.set_column('B:B', 5, cell_format=format_center)\n",
    "    worksheet.set_column('G:G', 5, cell_format=format_left)\n",
    "    worksheet.set_column('J:J', 5, cell_format=format_left)\n",
    "    worksheet.set_column('P:P', 6, cell_format=format_left)\n",
    "    worksheet.set_column('M:M', 5, cell_format=format_left)\n",
    "    worksheet.set_column('N:N', 7, cell_format=format_right)\n",
    "    worksheet.set_column('E:E', 6, cell_format=format_right)\n",
    "    worksheet.set_column('H:H', 6, cell_format=format_right)\n",
    "    worksheet.set_column('K:K', 7, cell_format=format_right)\n",
    "        #modify rows\n",
    "    worksheet.set_row((0),20, cell_format=format_boldL)\n",
    "    worksheet.set_row((1), cell_format=format_left)\n",
    "    worksheet.set_row((2), cell_format=format_left)\n",
    "    worksheet.set_row((3), cell_format=format_left)\n",
    "    worksheet.set_row((4), cell_format=format_left)\n",
    "    worksheet.set_row((6), 28, cell_format=format_boldCTR)\n",
    "    worksheet.set_row((no_grains+8), 20, cell_format=format_boldC)\n",
    "    worksheet.set_row((no_grains+11), cell_format=format_boldC)\n",
    "    worksheet.set_row((no_grains+12), cell_format=format_boldC)\n",
    "    worksheet.set_row((no_grains+13), cell_format=format_boldC)\n",
    "\n",
    "    workbook2 = writer.book\n",
    "    worksheet2 = writer.sheets['Variables'] \n",
    "    format_wrap2 = workbook2.add_format({'align': 'left', 'valign': 'top', 'text_wrap': True})\n",
    "    worksheet2.set_column('D:D', 100, cell_format=format_wrap2)\n",
    "\n",
    "    workbook3 = writer.book\n",
    "    worksheet3 = writer.sheets['Raw_Count_Data'] \n",
    "    format_wrap3 = workbook3.add_format({'align': 'left', 'valign': 'top', 'text_wrap': True})\n",
    "    worksheet3.set_column('G:G', 80, cell_format=format_wrap3)\n",
    "    writer.save()\n",
    "    ###############################################################################################################\n",
    "    #create a csv that will append pooled data, this will append new data (ie: next sample) as a new row so you can use the bulk data in other programs\n",
    "    append_pooled_data = pd.DataFrame({\"date_created\":datetime.now().strftime(\"%H:%M:%S %d/%m/%Y\"),\n",
    "                                        \"sample_name\":[sample_name],\n",
    "                                       \"analyst\":[analyst],\n",
    "                                       \"mineral\":[mineral],\n",
    "                                       \"rock_type\":[rock_type],\n",
    "                                       \"country\":[country],\n",
    "                                       \"region\":[region],\n",
    "                                       \"latitude\":[latitude],\n",
    "                                       \"longitude\":[longitude],\n",
    "                                       \"elevation\":[elevation],\n",
    "                                       \"Ustandard\":[Ustandard],\n",
    "                                       \"spot_size\":[spot_size],\n",
    "                                       \"lab_name\":[lab_name],\n",
    "                                       \"etchant\":[etchant],\n",
    "                                       \"etching_time\":[etching_time],\n",
    "                                       \"etching_temp\":[etching_temp],\n",
    "                                       \"Xi\":[round(Xi, 7)],\n",
    "                                       \"grains_tot\":[ns.count()],\n",
    "                                       \"ns_tot\":[np.sum(ns).astype(int)],\n",
    "                                       \"area_tot\":[round(np.sum(area),10)],\n",
    "                                       \"ps_mean\":[round(np.mean(ps),2)],\n",
    "                                       \"uppm_mean\":[round(np.mean(U_ppm),2)],\n",
    "                                       \"uppm_sdm\":[round(np.std(U_ppm), 2)],\n",
    "                                       \"euppm_mean\":[round(np.mean(eU), 2)],\n",
    "                                       \"euppm_sdm\":[round(np.std(eU), 2)],\n",
    "                                       \"page\":[round(Pooled_age, 2)],\n",
    "                                       \"page_sd\":[round(Pooled_age_1sigma, 2)],\n",
    "                                       \"cage\":[round(Central_age, 2)],\n",
    "                                       \"cage_sd\":[round(Central_age_1sigma, 2)],\n",
    "                                       \"X2\":[round(X2, 2)],\n",
    "                                       \"PX2\":[round(PX2_perc, 2)],\n",
    "                                       \"dispersion\":[round(Dispersion, 2)],\n",
    "                                       \"dpar_mean\":[round(np.mean(Dpar), 2)],\n",
    "                                       \"dpar_sdm\":[round(np.std(Dpar), 2)],\n",
    "                                       \"cl_wt_mean\":[np.where(Cl_sem_mean == '--', '', Cl_sem_mean)],\n",
    "                                       \"cl_sdm\":[np.where(Cl_sem_sdm == '--', '', Cl_sem_sdm)],\n",
    "                                       \"rmr0_mean\":[np.where(rmr0_mean == '--', '', rmr0_mean)],\n",
    "                                       \"rmr0_sdm\":[np.where(rmr0_sdm == '--', '', rmr0_sdm)],\n",
    "                                       \"rmr0D_mean\":[np.where(rmr0D_mean == '--', '', rmr0D_mean)],\n",
    "                                       \"rmr0D_sdm\":[np.where(rmr0D_sdm == '--', '', rmr0D_sdm)],\n",
    "                                       \"len_no\":[np.where(l_no == '--', '', l_no)],\n",
    "                                       \"mtl\":[np.where(mtl == '--', '', mtl)],\n",
    "                                       \"mtl_var\":[np.where(mtl_var == '--', '', mtl_var)],\n",
    "                                       \"mtl_sd\":[np.where(mtl_sd == '--', '', mtl_sd)],\n",
    "                                       \"dpar_length_mean\":[Dpar_lengths_mean],\n",
    "                                       \"rmr0D_length_mean\":[rmr0D_lengths_mean],\n",
    "                                       \"rmr0D_length_sdm\":[rmr0D_lengths_sdm]})\n",
    "\n",
    "    up_onefolder = str(Path(save_to).parents[0])\n",
    "    append_filepath = Path('{0}/pooled_age_summary.csv'.format(up_onefolder))\n",
    "\n",
    "    if append_filepath.exists():\n",
    "        append_pooled_df = pd.read_csv('{0}/pooled_age_summary.csv'.format(up_onefolder)) \n",
    "        append_pooled_df.drop(append_pooled_df[append_pooled_df['sample_name'] == sample_name].index,inplace=True, errors='raise')\n",
    "        append_pooled_df.to_csv('{0}/pooled_age_summary.csv'.format(up_onefolder), header=True, index=None)\n",
    "        append_pooled_data.to_csv('{0}/pooled_age_summary.csv'.format(up_onefolder), mode='a', header=False, index=None)\n",
    "    else:\n",
    "        append_pooled_data.to_csv('{0}/pooled_age_summary.csv'.format(up_onefolder), header=True, index=None)\n",
    "    ###############################################################################################################\n",
    "    #create and export tables for use in various programs\n",
    "        #create a folder to store these\n",
    "    data_folder = '{0}/Data_Import'.format(save_to)\n",
    "    if not os.path.exists(data_folder): os.makedirs(data_folder)\n",
    "    #export tables to use in HeFty\n",
    "        #Age File:\n",
    "        ###using dpar\n",
    "    hefty_age_header = pd.DataFrame({sample_name:[\"Zeta:\",\"zeta\",\"4352.32\",\"Ns\"],\n",
    "                                    \"col1\":[\"LAICPMS ratio\",\"sig zeta\",\"1.1\",\"Area (cm²)\"],\n",
    "                                    \"col2\":[\"\",\"\",\"\",\"Pcorr\"],\n",
    "                                    \"col3\":[\"\",\"\",\"\",\"sig(Pcorr)\"],\n",
    "                                    \"col4\":[\"\",\"\",\"\",\"Dpar\"]})   \n",
    "    hefty_age_data = pd.DataFrame({sample_name:ns,\n",
    "                                     \"col1\":area,\n",
    "                                     \"col2\":U_ppm,\n",
    "                                     \"col3\":U_ppm_1sigma,\n",
    "                                     \"col4\":Dpar}) \n",
    "    hefty_age_data = hefty_age_data.replace({\"col4\":{np.nan:Dpar_mean}}) \n",
    "    hefty_age = pd.concat([hefty_age_header, hefty_age_data], axis=0)\n",
    "    hefty_age.to_csv(r'{0}/{1}_HeftyAGE(Dpar).txt'.format(data_folder, sample_name), na_rep=0, index=None, sep='\\t')\n",
    "            ###using rmr0\n",
    "    if 'rmr0' in age_df.columns:\n",
    "        hefty_age_header_rmr0 = pd.DataFrame({sample_name:[\"Zeta:\",\"zeta\",\"4352.32\",\"Ns\"],\n",
    "                                    \"col1\":[\"LAICPMS ratio\",\"sig zeta\",\"1.1\",\"Area (cm²)\"],\n",
    "                                    \"col2\":[\"\",\"\",\"\",\"Pcorr\"],\n",
    "                                    \"col3\":[\"\",\"\",\"\",\"sig(Pcorr)\"],\n",
    "                                    \"col4\":[\"\",\"\",\"\",\"rmr0\"]})   \n",
    "        hefty_age_data_rmr0 = pd.DataFrame({sample_name:ns,\n",
    "                                     \"col1\":area,\n",
    "                                     \"col2\":U_ppm,\n",
    "                                     \"col3\":U_ppm_1sigma,\n",
    "                                     \"col4\":rmr0}) \n",
    "        hefty_age_data_rmr0 = hefty_age_data_rmr0.replace({\"col4\":{np.nan:rmr0_mean}}) \n",
    "        hefty_age_rmr0 = pd.concat([hefty_age_header_rmr0, hefty_age_data_rmr0], axis=0)\n",
    "        hefty_age_rmr0.to_csv(r'{0}/{1}_HeftyAGE(rmr0).txt'.format(data_folder, sample_name), na_rep=0, index=None, sep='\\t')\n",
    "\n",
    "    ################################\n",
    "        #Length File:\n",
    "    if lengths_file_w.value != {}:\n",
    "        hefty_length = pd.DataFrame({\"length\":lengths_data['True Length'],\n",
    "                    \"angle\":lengths_data['Angle to CAxis'],\"Dpar\":lengths_data['Average DPar(µmm)']})\n",
    "        hefty_length.to_csv(r'{0}/{1}_HeftyLENGTH(Dpar).txt'.format(data_folder, sample_name), na_rep=0, index=None, sep='\\t')\n",
    "    if lengths_file_w.value != {}:\n",
    "        hefty_length_rmr0 = pd.DataFrame({\"length\":lengths_data['True Length'],\n",
    "                    \"angle\":lengths_data['Angle to CAxis'],\"rmr0\":rmr0D_lengths})\n",
    "        hefty_length_rmr0.to_csv(r'{0}/{1}_HeftyLENGTH(rmr0).txt'.format(data_folder, sample_name), na_rep=0, index=None, sep='\\t')\n",
    "    ###############################################################################################################\n",
    "    #export tables to use in RadialPlotter\n",
    "        #ColorwUppm:\n",
    "    radialplotter_U = pd.DataFrame({sample_name:Age_sga.round(2),\"O\":Age_sga_1sigma.round(2),\"\":U_ppm.round(2)})\n",
    "    radialplotter_U.to_csv(r'{0}/{1}_Radial_U.csv'.format(data_folder, sample_name), na_rep=0, index=None)\n",
    "        #Colorw rmr0 calculated from Dpar\n",
    "    radialplotter_rmr0D = pd.DataFrame({sample_name:Age_sga.round(2),\"O\":Age_sga_1sigma.round(2),\"\":rmr0D})\n",
    "    radialplotter_rmr0D.to_csv(r'{0}/{1}_Radial_rmr0(From_Dpar).csv'.format(data_folder, sample_name), na_rep=0, index=None)\n",
    "        #ColorwCl (if cl is there)\n",
    "    if np.issubdtype(ns_df['Cl wt (%)'].dtype, np.number):\n",
    "        radialplotter_Cl = pd.DataFrame({sample_name:Age_sga.round(2),\"O\":Age_sga_1sigma.round(2),\"\":Cl_sem})\n",
    "        radialplotter_Cl.to_csv(r'{0}/{1}_Radial_Cl.csv'.format(data_folder, sample_name), na_rep=0, index=None)\n",
    "        #Colorwrmr0 (if rmr0 is there)\n",
    "    if np.issubdtype(ns_df['rmr0'].dtype, np.number):\n",
    "        radialplotter_rmr0 = pd.DataFrame({sample_name:Age_sga.round(2),\"O\":Age_sga_1sigma.round(2),\"\":rmr0})\n",
    "        radialplotter_rmr0.to_csv(r'{0}/{1}_Radial_rmr0.csv'.format(data_folder, sample_name), na_rep=0, index=None)\n",
    "    ###############################################################################################################\n",
    "    #export tables to use in QTQT (must copy and paste into QTQT)\n",
    "        #age\n",
    "    if 'rmr0' in age_df.columns:\n",
    "        qtqt_age = pd.DataFrame({'Sample:{0}\\n Ns'.format(sample_name): ns,\"ICPMS_Age\":Age_sga.round(2),\n",
    "                              \"ICPMS_Age 1Sigma\":Age_sga_1sigma.round(2),\"Dpar\":Dpar.round(2),\"Cl\":Cl_sem,\"rmr0\":rmr0})\n",
    "        qtqt_age.to_csv(r'{0}/{1}_QTQT_age.csv'.format(data_folder, sample_name), na_rep=0, index=None)\n",
    "    else:\n",
    "        qtqt_age = pd.DataFrame({'Sample:{0}\\n Ns'.format(sample_name): ns,\"ICPMS_Age\":Age_sga.round(2),\n",
    "                              \"ICPMS_Age 1Sigma\":Age_sga_1sigma.round(2),\"Dpar\":Dpar.round(2),\"Cl\":Cl_sem,\"rmr0\":rmr0D})\n",
    "        qtqt_age.to_csv(r'{0}/{1}_QTQT_age.csv'.format(data_folder, sample_name), na_rep=0, index=None)\n",
    "\n",
    "        #length\n",
    "    if lengths_file_w.value != {}:\n",
    "        qtqt_length = pd.DataFrame({'Sample:{0}\\n Length'.format(sample_name): lengths_data['True Length'],\"Angle to C\":lengths_data['Angle to CAxis'],\n",
    "                              \"Dpar\":lengths_data['Average DPar(µmm)'],\"rmr0_Dpar\":rmr0D_lengths})\n",
    "        qtqt_length.to_csv(r'{0}/{1}_QTQT_length.csv'.format(data_folder, sample_name), na_rep=0, index=None)\n",
    "    ###############################################################################################################\n",
    "    #export table to use in IsoplotR\n",
    "    isoplot_r = pd.DataFrame({'Sample:{0}\\n Ns'.format(sample_name): ns,\"A(µm2)\":(area*10**8).round(0),\n",
    "                              \"U1(ppm)\":U_ppm.round(2),\"err[U1]\":U_ppm_1sigma.round(2)})\n",
    "    isoplot_r.to_csv(r'{0}/{1}_IsoplotR.csv'.format(data_folder, sample_name), na_rep=0, index=None)\n",
    "    ###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counts_file_w.value != {} or counts_paths != []:\n",
    "    print('Done', date.today())\n",
    "    display(ns_df.style.hide_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
